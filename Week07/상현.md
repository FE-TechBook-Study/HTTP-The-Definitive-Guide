# 상현
# 8장 통합점: 게이트웨이, 터널, 릴레이

## 8.1 게이트웨이

> **(GPT의 쉬운 설명) 프록시 + 번역기**
> 

리소스가 다양해지면서 인터프리터 같이 리소스를 받기 위한 경로를 안내하는 역할을 하는 게이트웨이를 고안
게이트웨이는 HTTP 트래픽을 다른 프로토콜로 자동 변환 가능
다음 3가지 행동의 예:

- FTP URL을 가지는 HTTP → 게이트웨이 → FTP 서버와 연결 → FTP 응답을 받고 게이트웨이는 적절한 HTTP 헤더와 함께 문서를 클라이언트로 전달
- 암호화된 웹 요청을 게이트웨이가 받음 → 해독하여 일반 HTTP 요청으로 변경 → 목적지 서버로 전단
- HTTP 요청을 애플리케이션 서버 게이트웨이가 받음 → 관련 애플리케이션 프로그램에 연결

### 8.1.1 클라이언트 측 게이트웨이와 서버 측 게이트웨이

웹 게이트웨이는 한 쪽에서 HTTP로 통신하고 다른 쪽은 HTTP가 아닌 프로토콜로 통신
게이트웨이는 빗금(/)을 통하여 클라이언트와 서버 프로토콜을 구분
`<클라이언트 프로토콜>/<서버 프로토콜>`

역할에 따라서 다음 2가지로 분류

- 서버 측 게이트웨이: (즉, 사용되는 곳은 클라이언트가 서버로 보내는 시기)
    - 클라이언트와 HTTP로 통신
    - 서버와는 외래 프로토콜(foreign protocol)
- 클라이언트 측 게이트웨이: (즉, 사용되는 곳은 서버가 클라이언트로 보내는 시기)
    - 클리언트와 외래 프로토콜(foreign protocol)
    - 서버와 HTTP 프로토콜

## 8.2 프로토콜 게이트웨이

웹 브라우저에서 특정 URL에 대한 FTP URL을 HTTP/FTP 게이트웨이로 설정 가능하면 아래와 같이 동작

- 일반적인 HTTP는 웹 서버로 전달
    
    ```tsx
    GET http://www.xxxx.com HTTP/1.0
    HOST: www.xxx.com
    ```
    
- FTP URL 포함한 경우에는 특정 게이트웨이로 HTTP 요청
    
    ```tsx
    GET frp://ftp.xxx.xxx.xxxx HTTP/1.0
    HOST: ftp.xxx.xxx
    ```
    

### 8.2.1 HTTP/*: 서버 측 게이트웨이

웹 클라이언트로부터 HTTP 요청이 원 서버로 들어오는 시점에 외래 프로토콜로 전환
아래와 같은 일들을 수행

- USER와 PASS 명령을 보내서 서버에 로그인
- 서버에서 적절한 디렉터리로 변경하기 위해 CWS 명령
- 다운로드 형식을 ASCII로 설정
- MDTM으로 문서의 최근 수정 시간 탐색
- PASV로 서버에게 수동형 데이터 검색하겠다고 말하기
- RETR로 객체 검색
- 제어 채널에서 반환 포트로 FTP 서버에 데이터 커넥션을 맺고 채널이 열리면 객체가 게이트웨이로 전송

게이트웨이는 객체를 받는 대로 HTTP 응답에 실어서 클라이언트에 전송

### 8.2.2 HTTP/HTTPS: 서버 측 보안 게이트웨이

모든 웹 요청을 암호화함으로써 개인 정보 보호와 보안 제공
클라이언트가 일반 HTTP를 게이트웨이에 전달 → 게이트웨이는 사용자의 모든  세션을 암호화

### 8.2.3 HTTPS/HTTP: 클라리언트 측 보안 가속 게이트웨이

웹 서버 앞단에 위치하고, 보이지 않는 인터셉트 게이트웨이나 리버스 프락시 열할 수행
보안 HTTPS 트래픽을 받고 복호화하고, 웹서버에 일반 HTTP 요청 전달

원 서버의 복호화 부하를 줄여주기는 하지만 원 서버와는 암호화되지 않은 트래픽을 하기에 네트워크의 안전을 확실히 확인하고 사용

## 8.3 리소스 게이트웨이

게이트웨이의 일반적인 형태는 HTTP 요청을 받아 내부 애플리케이션과 통신해 응답을 생성하는 애플리케이션 서버 구조

공용 게이트웨이 인터페이스(CGI)는 특정 URL에 대한 HTTP 요청에 따라 프로그램을 실행하고 HTTP 응답으로 회신하는데 사용하는 인터페이스 집합

### 8.3.1 공용 게이트웨이 인터페이스(Common Gateway Interface, CGI)

> **서버 확장:** 정적 HTML만 제공하는 것이 아닌 입력 처리, 외부 프로그램 실행, 동적 HTML 생성이 가능하도록 기능적 확장
> 

공용 게이트웨이 인터페이스(CGI)는 최초의 서버 확장이자 지금까지도 가장 널리 쓰이는 서버 확장
수많은 언어로 구현이 가능하며 단순하여 거의 모든 HTTP 서버가 지원
확장으로 다양한 기능을 수행하지만, 성능 관련한 비용이 발생

1. 모든 CGI 요청마다 새로운 프로세스 생성 필요
2. CGI를 사용하는 서버의 성능을 제한하며 서버 장비에 부담

위를 해결하기 위해, Fast CGI를 개발.
CGI와 유사하지만 새로운 프로세스를 만들고 제거하면서 이전 버전의 성능 저하 문제(위 1번) 해결

### 8.3.2 서버 확장 API

> 자세한 내용은 19장에서 확인
> 

CGI를 활용하여 서버를 확장을 헀지만, 서버 자제의 동작을 바꾸거나 서버의 처리능력을 최고치롤 끌어올리는 것을 원한다면 서버 확장 API를 활용 가능

## 8.4 애플리케이션 인터페이스와 웹 서비스

웹 서비스는 처음엔 **독립형 웹 애플리케이션(=building block)** 개념으로 시작됐고, 이후 **서로 정보 교환** 하는 **공식적 메커니즘** 으로 자리잡음

## 8.5 (웹) 터널

웹 터털은 HTTP 프로토콜을 지원하지 않는 애플리케이션에 HTTP 애플리케이션을 사용해 접근하는 방법 제공
사용하는 가장 일반적인 이유는 HTTP 커넥션 안에 HTTP가 아닌 트래픽을 사용할 수 있게 하기 위해서

### 8.5.1 CONNECT로 HTTP 터널 커넥션 맺기

HTTP의 CONNECT 메서드를 사용하여 터널 커넥션 생성
HTTP 1.1 명세에 자세히 나와 있지는 않지만, 터널 게이트웨이가 임의이 목적 서버와 포트에 TCP 커넥션을 맺고 클라이언트와 서버 간에 오는 데이터를 무조건 전달하기로 요청

![스크린샷 2025-07-23 오후 3.07.15.png](https://img.notionusercontent.com/s3/prod-files-secure%2F6eac9da7-a6e6-4d98-84db-37a4eaddb1d5%2Fdb411203-5011-4f90-a9ce-f3f1f0d721a1%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-07-23_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.07.15.png/size/w=1920?exp=1753685976&sig=g4W1HGcnH_XtqKNCYkYPnvuX45nrVl4t_PmH9YNSrEk&id=239438a5-b001-8088-8483-ffc628bee816&table=block&userId=fa8dc9c2-7ea8-437a-8bc2-3fbca5c283fb)

- (a)에서 CONNECT를 통하여 연결을 요청
- (b)와 같이 게이트웨이가 서버에 TCP 연결을 하고 성공하면 (c)와 같이 반환
- (d)와 같이 클라이언트에 200 Connection Established를 보내어 연결 성공 알림
- 터널이 열리고 데이터들이 전송

**CONNECT 요청**

다른 HTTP 요청과 동일하게 아래와 같이 요청

```tsx
CONNECT home.netscape.com:443 HTTP/1.0
User-agent: Mozilla/4.0
```

**CONNECT 응답**

200 응답 코드와 사유구절은 Connection Established로 기술
콘텐츠 형식을 기술하는 Content-Type 헤더 불필요.

```tsx
HTTP/1.0 200 Connection Established
Proxy-agent: Netscape-Proxy/1.1
```

### 8.5.2 데이터 터널링, 시간, 커넥션 관리

클라이언트는 성능을 높이기 위해 CONNECT 요청을 보낸 다음 응답 받기 전 터널 데이터 전송 가능.
단, 게이트웨이가 요청에 이이서 데이터를 적절하게 처리할 수 있어야 함을 전제

게이트웨이가 받은 네트워크 I/O 요청이 헤더만 반환해줄 거라고 가정할 수 없어서, 커넥션이 맺어진 대로 헤더를 포함해서 읽어드린 데이터를 서버에 전송하고 200외의 응답에 대응 필요

터널 어느 끝단에서 커넥션이 끊기기 이전 값들은 반대편으로 전달되고, 끊긴 이후로 전송하지 않은 데이터는 버려짐

### 8.5.3 SSL 터널링

> **내가 정리한 웹 터널 프로세스**
> 
> - 내가 실제로 하고 싶은 것은 FTP 요청
> - 그러나 FTP 서버 앞에 방어막이 있고 FTP 요청 불가능
> - FTP 요청을 HTTP로 감싸고 HTTP 요청을 보냄
> - 서버 방어벽은 HTTP만 받기에 HTTP를 수용하고
> - 방어벽 바로뒤에는 게이트웨이 서버가 있고 들어온 HTTP 메세지를 해독하고 FTP요청이 원래 요청임을 알게됨
> - FTP 요청을 원 FTP 서버로 보냄
> - FTP에서 정보를 읽으면 서버는 HTTP응답을 만들고 돌려보냄

![스크린샷 2025-07-23 오후 5.01.11.png](https://img.notionusercontent.com/s3/prod-files-secure%2F6eac9da7-a6e6-4d98-84db-37a4eaddb1d5%2F2d59e1c0-31ec-4ca5-b928-b129b72d042b%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-07-23_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.01.11.png/size/w=1920?exp=1753685997&sig=-jOtMdxaPPHaiJsBrA___88_0b7B8ZiwiL_ZhEKs78g&id=239438a5-b001-804f-bdf0-c5e96902ca19&table=block&userId=fa8dc9c2-7ea8-437a-8bc2-3fbca5c283fb)

웹 터널은 원래 방화벽을 통해서 암호화된 SSL 트래픽을 전달하려고 개발

### 8.5.4 SSL 터널링 vs HTTP/HTTPS 게이트웨이

???????

### 8.5.5 터널 인증

HTTP의 다른 기능들은 터널과 함께 적절히 사용가능
예를 들어, 프락시 인증 기능을 활용하여 클라이언트가 터널을 사용할 수 있는 권한이 있는지 체크 가능

### 8.5.6 터널 보안에 대한 고려사항들

터널 게이트웨이를 사용하고 있는 프로토콜들이 올바른 용도로 사용하고 있는지 검증하기 어려움.
따라서, 오용을 최소화하기 위해서 게이트웨이는 HTTPS 전용 포트인 443같이 잘 알려진 특정 포트만 터널링 허용

## 8.6 릴레이

릴레이란 HTTP 명세를 완전히 준수하지 않는 간단한 HTTP 프락시
커넥션을 맺기 위해 HTTP 통신 후 바이트를 맹목적으로 전달

위 특성으로 인해, 모든 헤더와 로직을 수행하지 않고 트래픽을 전달하지만 `Connection: Keep-Alive`를 이해하지 못하여 발생하는 문제가 존재:

![스크린샷 2025-07-23 오후 5.53.20.png](https://img.notionusercontent.com/s3/prod-files-secure%2F6eac9da7-a6e6-4d98-84db-37a4eaddb1d5%2Fd6089e57-3906-4cd0-b6f1-a248528f6e72%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-07-23_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.53.20.png/size/w=1920?exp=1753686011&sig=llub9-KrOQ7gcT9lSqFW9GPEA9mvDUhKkn2KO4lCqyE&id=239438a5-b001-807e-9dd3-ec5eeaabfc02&table=block&userId=fa8dc9c2-7ea8-437a-8bc2-3fbca5c283fb)

- e의 요청을 받았는데 릴레이는 Connection 헤더를 이해하지 못하기에, 하나의 커넥션에 하나의 통신으로 알고 같은 커넥션에 받은 요청에 대해서 무시

# 9장 웹 로봇

## 9.1 크롤러와 크롤링

웹 크롤러(또는 스파이더)란 한 개의 웹 페이지 안에 있는 다른 웹 페이지들을 재귀적으로 순회하는 로봇

### 9.1.1 어디에서 시작하는가: ‘루트 집합’

루트 집합이란 크롤러가 방문을 시작하는 URL들의 초기 집합
어떤 루트 집합이 좋은가에 대해서는 완벽한 것은 없다. 실제로 모든 문서로 이어지게 되는 하나의 문서가 없기 때문이다.

일반적으로 좋은 루트 집한은 다음 목록을 구성

- 크고 인기 있는 웹 사이트
- 새로 생성된 페이지들의 목록
- 자주 링크되지 않는 잘 알려져 있지 않은 페이지

### 9.1.2 링크 추출과 상대 링크 정상화

크롤러는 페이지들 안에 있는 URL 링크들을 파싱해서 크롤링 리스트에 올리고 이 리스트들을 다시 크롤링하며 반복

### 9.1.3 순환 피하기

로봇들은 순환을 방지하기 위해서 방문한 페이지의 정보를 기록 필요

### 9.1.4 루프와 중복

순환은 다음 3가지 이유로 크롤러에게 해롭

- 루프에 빠지면 크롤러가 네트워크 대역폭을 다 차지하여 웹 페이지를 가져올 수 없게 될 수 있음
- 같은 페이지의 호출은 웹 서버의 부담
- 크롤러가 중복된 페이지들로 넘쳐나게 됨

### 9.1.5 빵 부스러기의 흔적

방문한 URL의 정보를 기억하려면 대용량의 저장소가 필요하며 활용하려면 빠른 속도를 제공하는 검색 구조(자료 구조)가 필요
몇 가지의 유용한 기법으로

- 트리와 해시 테이블
- 느슨한 존재 비트맵(pesence bit array)
    - 해시 함수로 인하여 URL은 고정된 크기와 숫자로 변환되고 배열 안에 ‘존재 비트’를 갖게 됨
    - 존재 비트가 있다면 이미 크롤링 되었다고 간주
- 체크포인트
    - 로봇이 갑자기 중간될 경우를 대비해 방문한 URL의 목록이 디스크에 있는 지 점검
- 파티셔닝
    - 한 대의 컴퓨터가 모든 웹을 수징하는 것은 불가능 하기에 동시에 로봇을 돌려서 서로 정보를 같이 조합하여 저장

### 9.1.6 별칭과 로봇 순환

URL이 별칭을 가질 수 있기에 이 또한 확인해서 저장 필요

![스크린샷 2025-07-25 오후 5.42.16.png](https://img.notionusercontent.com/s3/prod-files-secure%2F6eac9da7-a6e6-4d98-84db-37a4eaddb1d5%2Fe57cbeac-cf68-4b1c-946e-cb968fc8347c%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-07-25_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.42.16.png/size/w=1920?exp=1753686030&sig=wmubVWQa3smwqqVbHyPpad_BtuRBDwRh3MJXvCP4Diw&id=23b438a5-b001-80d0-aa11-d0b4276849a0&table=block&userId=fa8dc9c2-7ea8-437a-8bc2-3fbca5c283fb)

### 9.1.7 URL 정규화하기

다음과 같은 방식으로 모든 URL을 정규화

- 포트 번호가 명시되지 않았다면 ‘:80’을 추가
- 모든 %xx 이스케이핑된 문자들을 대응 문자로 변환
- #태그 제거

이렇게 졍규화를  시키면 이미 같은 URL에 대한 리소스를 구별하고 활용 가능
그러나 안 되는 케이스도 존재(위 표9-1의 d, e, f)

### 9.1.8 파일 시스템 링크 순환

![스크린샷 2025-07-26 오후 3.16.34.png](https://img.notionusercontent.com/s3/prod-files-secure%2F6eac9da7-a6e6-4d98-84db-37a4eaddb1d5%2F32f48392-2c5d-48c0-a834-5e7195431104%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-07-26_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.16.34.png/size/w=1920?exp=1753686046&sig=Eid4mHp7KAlN6eh5Wx4Ud05WkdniLl_BtbJly-4j0r4&id=23c438a5-b001-800b-a186-df8102217866&table=block&userId=fa8dc9c2-7ea8-437a-8bc2-3fbca5c283fb)

심벌릭 링크는 끝없는 순환을 유발할 수 있다

- 9-3(b)를 보면 /subdir/index.html을 불러왔지만 다시 root로 가고
- 위 동작을 계속 반복

로봇은 이 것을 인지하지 못 하면 로봇이나 서버의 한계가 넘을 때까지 순환이 반복

### 9.1.9 동적 가상 웹 공간

악이적인 웹 서버나 웹 마스터들이 매번 새 URL을 만들어내거나 날조하여 크롤러가 무한 루프에 빠지게 만들 수 있다.

### 9.1.10 루프와 중복 피하기

로봇이 더 올바르게 동작하기 위해서 다음 기법들을 사용

- URL 정규화
    - URL을 표준 형태로 변환하여 중복 URL 방지
- 너비 우선 크롤링
    - 방문할 URL들을 너비 우선 스케줄링을 하면 순환의 영향을 최소화 가능
    - 순환에 빠지기 전에 다른 웹 사이트들을 받기 가능
- 스로틀링
    - 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한
- URL 크기 제한
    - 일정 길이(보통 1KB)를 넘는 URL의 크롤링 거부 가능
        - 순환으로 길어지는 URL을 길이로 제한
    - 그러나, URL에 사용자의 상태를 저장하는 사례도 있기에 단점도 존재
- URL/사이트 블랙리스트
    - 순환을 만들어내는 URL 목록을 만들어 관리하여 방지
    - 사람이 직접 작성
- 패턴 발견
    - 오설정을 만드는 패턴을 보고 비슷한 URL 크롤링을 방지
        - 반복되는 구성요소
- 콘텐츠 지문
    - 페이지의 콘텐츠를 활용하여 체크섬(checksum)을 계산하고 보관하고 있다가 방문한 페이지의 체크섬과 동일하다면 크롤링 X
    - 체크섬을 생성할 때 동적으로 변경되는 콘텐츠는 계산에서 뺴고 진행
- 사람의 모니터링
    - 새로운 규칙은 항상 만들어지고 기존 규칙들이 진화를 하지만 동시에 새로운 리소스가 만들어짐에 따라 사람의 모니터링을 통해소 일부분 필터링

## 9.2 로봇의 HTTP

로봇은 HTTP로 통신을 하며 요구사항이 적은 HTTP 1.0을 이용

### 9.2.1 요청 헤더 식별하기

잘못된 크롤러를 찾아내고 로봇이 활용할 수 있는 콘텐츠 종류를 알 수 있게 요청 시 권장되는 식별 헤더는 아래:

- User-Agent: 요청하는 로봇의 이름 제공
- From: 로봇의 사용자/관리자의 이메일 주소 제공
- Accept: 서버에게 어떤 미디어 타입을 보내도 되는지 제공
- Referer: 현재의 요청 URL을 포함한 문서의 URL 제공

### 9.2.2 가상 호스팅

가상 호스팅되어 있는 페이지들을 위해 Host헤더를 포함하는 것을 권장
만약 Host값이 없다면 “두 개의 사이트를 운용하는 서버”는 디폴트로 설정되어 있는 document를 전송

### 9.2.3 조건부 요청

캐시와 마찬가지로 로컬에 가기고 있는 값을 비교하기 위해 조건부 HTTP 요청으로 변경점이 있는지 확인

### 9.2.4 응답 다루기

대부분 GET 요청과 응답이기에 응답을 다루는 일은 없지만, 웹 탐색이나 서버와의 상호작용을 더 잘해보려는 로봇들은 여러 종류의 HTTP 응답 기능 필요

- 상태코드
    - 최소한 일반적인 상태 코드나 예상할 수 있는 상태 코드를 다룰 수 있어야 함
    - 또한, 모든 서버가 항상 적절한 에러 코드를 반환하지 않기에 알고 있을 필요(?)
- 엔터티
    - HTTP 헤더나 HTML 문서의 HEAD 태그를 인식하고 활용

### 9.2.5 User-Agent 타기팅(targeting)

사이트 관리자들은 최소한 로봇이 사이트에 방문했을 시 콘텐트를 얻을 수 없어 당황하는 일이 없도록 대비
그러기 위해서 로봇의 요청을 다루기 위한 전략이 필요.

## 9.3 부적절하게 동작하는 로봇들

- 폭주하는 로봇
    - 로봇은 사람보다 훨씬 빠르게 HTTP 요청을 만들고 빠른 네트워크 연결을 사용
    - 로봇이 폭주하여 웹 서버에 부하를 주는 것을 방지하기 위해서 보호 장치 설계 필요
- 오래된 URL
    - URL 목록의 사이트 방문 시, 존재하지 않는 문서에 대한 접근을 하면 에러 로그나 페이지로 제공하는 것은 웹 서버의 수용 능력 감소로 연결
- 길고 잘못된 URL
    - 순환으로 인하여 크고 의미없는 URL 요청할 수 있으며 웹 서버의 수용 능력 감소시키고 고장 발생 가능
- 호기심이 지나친 로봇
    - 사적인 URL을 크롤링하여 이를 검색하고 접근할 수 있게 제공하는 크롤링 로봇 존재 가능
    - 이런 행동을 방지하기 위한 메커니즘이 필요
- 동적 게이트웨이 접근
    - 로봇은 접근하는 곳에 대한 정보를 언제나 잘 알고 있지 않기에, 동적 게이트웨이의 특성을 모르고 모든 URL을 조회 요청하는 행동 가능

## 9.4 로봇 차단하기

로봇이 접근 제어에 대한 기법으로 “Robots Exclusion Standard”라고 이름 지어졌지만 저장하는 파일의 이름을 따라 robots.txt로 불림
로봇은 페이지 요청 전 robots.txt를 확인하여 페이지 접근 권한을 확인 필요

### 9.4.1 로봇 차단 표준

임시방편으로 마련된 표준이기에 모두가 따르지 않으나 없는 것보다는 훨씬 낫고 주류 검색엔진들은 이 표준을 지원

다음 3가지 버전이 있으며, 대부분 V0.0이나 V1.0을 채택
V2.0은 복잡하기에 채택되지 않았고 앞으로도 그럴 예정

| 버전 | 이름과 설명 | 날짜 |
| --- | --- | --- |
| 0.0 | 로봇 배제 표준-Disallow를 지시자를 지원하는 마틴 코스터의 오리지널 robots.txt 메커니즘 | 1994년 6월 |
| 1.0 | 웹 로봇 제어 방법-Allow 지시자의 지원이 추가된 마틴 코스터의 IETF 초안 | 1996년 11월 |
| 2.0 | 로봇 차단을 위한 확장 표준-정규식 타이밍 정보를 포함한 숀 코너의 확장. 널리 지원 X | 1996년 11월 |

### 9.4.2 웹 사이트와 robots.txt 파일들

웹 사이트 별로 하나씩 존재

- robots.txt 가져오기
    - GET 요청을 통해서 robots.txt를 가져오고, 존재하지 않는다면 404 Not Found로 응답
    
    ```tsx
    GET /robots.txt HTTP/1.0
    Host: xxxxxxxx
    User-Agent: XXXXX
    Date: XXXXXXXX
    ```
    
- 응답 코드
    - 파일이 존재한다며 파싱하여 차단 규칙을 얻고 규칙에 따라 접근
    - 404 Not Found라면, 제약 없이 접근
    - 서버가 접근제한(401, 403)으로 응답하면 로봇은 사이트 접근이 완전히 제한
    - 일시적 실피라면(503) 리소스 검색을 나중에 다시 시도
    - 응답이 리다이렉션(30X)라면 리소스 발견될 때까지 리다이렉트 따라감

### 9.4.3 robots.txt 파일 포멧

빈 줄, 주석 줄, 규칙 줄 세가지 종류로 이루져 있고 규칙 줄은 아래와 같이 표현:

```tsx
# robots.txt file for YouTube
# Created in the distant future (the year 2000) after
# the robotic uprising of the mid 90's which wiped out all humans.

User-agent: Mediapartners-Google*
Disallow:

User-agent: *
Disallow: /api/
Disallow: /comment
Disallow: /feeds/videos.xml
Disallow: /file_download
Disallow: /get_video
Disallow: /get_video_info
Disallow: /get_midroll_info
Disallow: /live_chat
Disallow: /login
Disallow: /qr
Disallow: /results
Disallow: /signup
Disallow: /t/terms
Disallow: /timedtext_video
Disallow: /verify_age
Disallow: /watch_ajax
Disallow: /watch_fragments_ajax
Disallow: /watch_popup
Disallow: /watch_queue_ajax
Disallow: /youtubei/

Sitemap: https://www.youtube.com/sitemaps/sitemap.xml
Sitemap: https://www.youtube.com/product/sitemap.xml

```

- User-agent 뒤에 Allow와 Disallow 줄이 따라오는 형태
- User-agent
    - 설정하려고 하는 로봇의 이름 기재
    - 이름이 매칭되는 첫 번째이거나, ‘*’ 중에 첫 번째 규칙을 사용
    - 로봇이 요청보내는 User-agent의 값에 해당이 되는 것이 존재한다면 해당 아래에 있는 Allow와 Disallow 규칙 준수. 매칭이 없다면 모든 접근 허용
    - 로봇의 이름 매칭 시 대소문자에 상관없이 하기에 주의 필요
- Allow & Disallow
    - 로봇에 대해 어떤 URL 경로가 명시적으로 금지 또는 허용이 되는지 기술
    - 첫 번째로 매칭되는 규칙을 사용
- Allow & Disallow 접두 매칭(prefix matching)
    - 규칙이 적용되려면 경로의 시작부터 경로의 길이만큼 문자열 규칙 경로가 (대소문자 차이없이)동일
    - 이스케이핑된 문자들은 원래대로 복원 후 비교
    - 어떤 규칙 경로가 빈 문자열이면 그 규칙은 모든 URL경로와 매디ㅣㄴ
        
        ![스크린샷 2025-07-28 오후 2.01.10.png](https://img.notionusercontent.com/s3/prod-files-secure%2F6eac9da7-a6e6-4d98-84db-37a4eaddb1d5%2F267f75d7-67e9-4add-a29d-61a1240329fb%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-07-28_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.01.10.png/size/w=1920?exp=1753686065&sig=NLXSB0NfXD_1N2ZRx5wXgcmBKESMAIoLoqiZtStv-EQ&id=23e438a5-b001-8019-aa91-d065db52eb90&table=block&userId=fa8dc9c2-7ea8-437a-8bc2-3fbca5c283fb)
        
- 규정에 대한 단점:
    - 특정 디렉토리 하위는 전부 접근근지를 시키고 싶지만, 지원하는 방법이 없기에 하위 모든 URL 기재 필요

### 9.4.4 그 외에 알아둘 점

- robots.txt가 발점함에 따라 User-agent, Allow, Disallow 외의 다른 필드가 추가될 수 있고, 로봇은 자신이 이해하지 못하는 필드는 무시
- 하위 호환성을 위해 한 줄을 여러 줄로 나누어 적는 것은 허용X
- 주석은 어디에든 작성 가능
- 버전 0.0은 Allow를 지원하지 않기에 이를 인지하고 작성

### 9.4.5 robots.txt의 캐싱과 만료

robots.txt에 대한 캐싱 정보는 HTTP 헤더에 포함하기에 로봇은 헤더의 정보들을 보고 파악하여 요청 조절
또한, 많은 크롤러는 HTTP 1.1이 아니기에 캐시 지시자를 이해 못할 수 있다는 점을 유의

> 2025년 7월 기준으로 크롤러는 HTTP 1.1이상을 지원 가능한 것으로 추측가능. [Cache-Control 헤더를 인식하고 사용한다는 Google 문서](https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt?hl=ko)
> 

> Youtube의 robots.txt를 확인했을 때 HTTP 3.0을 지원하여 불러오는 것을 확인 가능
> 

![스크린샷 2025-07-28 오후 2.27.44.png](https://img.notionusercontent.com/s3/prod-files-secure%2F6eac9da7-a6e6-4d98-84db-37a4eaddb1d5%2F6fa6fb72-a68a-4b84-b234-06f3cdc74e17%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-07-28_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.27.44.png/size/w=1920?exp=1753686084&sig=9srpcs7dWxoWR6rjbTgikVHBZPwvyCXqhF28_ELISiE&id=23e438a5-b001-800b-b5e7-c834ca96d910&table=block&userId=fa8dc9c2-7ea8-437a-8bc2-3fbca5c283fb)

### 9.4.6 로봇 차단 펄 코드

펄 라이브러리를 통하여 robots.txt 파일을 파싱할 수 있다

### 9.4.7 HTML 로봇 제어 META 태그

HTML HEAD에 추가 가능하며 대소문자 구분X
지시자들을 중복해서 기재 X
시간이 지남에 따라 새 지시자가 추가될 수 있지만 가장 널리 쓰이는 두 가지는:

- NOINDEX `<META NAME="ROBOTS" CONTENT="NOINDEX">`
    - 이 페이지를 처리하지 말고 무시 → 페이지의 콘텐츠 크롤링 X
- NOFOLLOW `<META NAME=”ROBOTS” CONTENT=”NOFFOLLOW”>`
    - 이 페이지가 링크한 페이지를 크롤링 X
- 그 외 추가적으로:
    - INDEX: 이 페이지 처리 인덱싱 가능
    - FOLLOW: 이 페이지가 링크한 페이지 크롤링 가능
    - NOARCHIVE: 캐시를 위한 로컬 사본 생성 X
    - ALL: INDEX + FOLLOW 통합
    - NONE: NOINDEX + NOFOLLOW 통합

검색엔진 META 태그(이 지시자들은 널리 지원되지 않는 듯…)

![스크린샷 2025-07-28 오후 2.39.21.png](https://img.notionusercontent.com/s3/prod-files-secure%2F6eac9da7-a6e6-4d98-84db-37a4eaddb1d5%2F2715c1e9-6468-4aa5-aed3-0de3d5bf32b9%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-07-28_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.39.21.png/size/w=1920?exp=1753686111&sig=57-z_xA2mhpaD_N3ejeazz1JbxBGtBhRDVg1LmSITyM&id=23e438a5-b001-8057-baaa-ed715afb46ab&table=block&userId=fa8dc9c2-7ea8-437a-8bc2-3fbca5c283fb)

## 9.5 로봇 에티켓

[Guidelines for Robot Writers](https://www.robotstxt.org/guidelines.html)는 오래된 작성 규칙이지만 아직도 유용한 가이드라인으로서 참고

## 9.6 검색엔진

검색엔지이 어떻게 동작하는지 확인

### 9.6.1 넓게 생각하라

검색엔진을 통해서 생성되는 페이지들은 웹 크롤러가 동작 후 만들어지는데 순차적으로 처리하면 시간 소요가 매우 큼. 따라서, 많은 장비들을 사용하여 요청을 병렬 처리를 진행

### 9.6.2 현대적인 검색엔진의 아키텍처

검색엔진들은 전 세계 웹페이지에 대해 ‘풀 텍스트 색인(full-text indexes)’라는 로컬 데이터 베이스 생성
크롤러들이 웹 페이지를 수잡히여 색인에 추가하고 유저 검색이 발동 시, 풀 텍스트 색인에 질의를 보내고 답변을 유저에게 전달

페이지는 매 순간 변화하기 때문에, 데이터베이스는 특정 순간에 대한 스냅샵에 불과

### 9.6.3 풀 텍스트 색인

입력된 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스
색인이 생성되었다면 검색할 필요X

![스크린샷 2025-07-28 오후 3.22.25.png](https://img.notionusercontent.com/s3/prod-files-secure%2F6eac9da7-a6e6-4d98-84db-37a4eaddb1d5%2F26b6bcad-7ca1-42cf-842e-1c0e2cbb8d9c%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-07-28_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.22.25.png/size/w=1920?exp=1753686138&sig=Ma6SEDc7ZuY7LUhAMxljTlaV3Z_3zP__pJB_7GCO15U&id=23e438a5-b001-8003-b51b-f730b66e3e90&table=block&userId=fa8dc9c2-7ea8-437a-8bc2-3fbca5c283fb)

### 9.6.4 질의 보내기

사용자의 요청이 GET이나 POST에 담겨져져서 게이트웨이로 전달되면, 검색 질의를 추출하여 풀 텍스트 색인에 요청할 수 있는 표현식으로 변환 후 검색 시작

검색 게이트웨이는 결과 값을 웹 서버에 반환하고 이를 HTML로 만들어서 유저에게 응답

### 9.6.5 검색 결과를 정렬하고 보여주기

관련도 랭킹을 통하여 문서들이 주어진 단어와 가장 관련이 많은  순서대로 문서들 간의 순서를 정하고 검색 결과를 정렬

위 과정을 더 잘 지원하기 위해 크롤링하는 과정에서 수집된 통계 데이터를 사용
예를 들어, 어떤 페이지가 링크되어 있는 개수를 판별하여 인기도를 판별하여 정렬 순서에 대한 가중치를 추가 가능
자세한 기교는 검색엔지 고유의 비밀들로 존재

### 9.6.6 스푸핑(Spoofing)

검색결과의 상단에 위치하려고 웹 사이트들은 여러 기법을 활용
그러나, 관련이 없는 단어를 추가하여 검색 알고리즘을 잘 속이거나 특정 단어에 대한 가짜 페이지를 만드는 행위가 있음

검색엔진과 로봇 구현자들은 이러한 속임수(spoofing)을 잡아내기 위해서 검색 알고리즘을 끊임없이 수정 필요
