# 민지

# 8. 통합점: 게이트웨이, 터널, 릴레이

HTTP는 웹에 있는 모든 리소스에 대한 프로토콜로 사용됐으며, 애플리케이션 간에 서로 다른 프로토콜을 상호 운용하는 용도로 사용하기도 한다.

* 게이트웨이: 서로 다른 프로토콜과 애플리케이션 간의 HTTP 인터페이스다.
* 애플리케이션 인터페이스: 서로 다른 형식의 웹 애플리케이션이 통신하는 데 사용한다.
* 터널: HTTP 커넥션을 통해서 HTTP가 아닌 트래픽을 전송하는 데 사용한다.
* 릴레이: 일종의 단순한 HTTP 프락시로, 한 번에 한 개의 홉에 데이터를 전달하는 데 사용한다.

## 8.1 게이트웨이

* 웹에 더 복잡한 리소스를 올릴 필요가 생기면서 모든 리소스를 한 개의 애플리케이션으로만 처리하기 어려워졌다.
* 인터프리터 같이 리소스를 받기 위한 경로 안내 역할의 게이트웨이를 고안해냈다.
* 게이트웨이는 리소스와 애플리케이션을 연결하는 역할을 한다.
  * 애플리케이션은 게이트웨이에게 요청을 처리해달라고 할 수 있다(HTTP 또는 그 외 정의해 둔 인터페이스를 통해).
  * 게이트웨이는 그에 응답할 수 있으며, 동적인 콘텐츠를 생성하거나 데이터베이스에 질의를 보낼 수 있다.
  * 웹사이트의 서버는 데이터베이스로 가는 게이트웨이 역할을 할 수 있다. 클라이언트가 HTTP를 통해 리소스를 요청하고, 서버는 리소스를 얻기 위한 게이트웨이 인터페이스 역할을 한다.
* 게이트웨이는 HTTP 트래픽을 다른 프로토콜로 자동 변환하여, HTTP 클라이언트가 다른 프로토콜을 알 필요 없이 서버에 접속할 수 있게 한다.
* 게이트웨이의 예
  * HTTP/FTP 서버 측 FTP 게이트웨이
    * 게이트웨이는 FTP URL을 가리키는 HTTP 요청을 받아 FTP 커넥션을 맺고, FTP 서버에 적절한 명령을 전송한다.
    * 클라이언트는 적절한 HTTP 헤더와 함께 문서를 받는다.
  * HTTPS/HTTP 클라이언트 측 보안 게이트웨이
    * SSL을 통해 암호화된 웹 요청을 받고, 요청을 해독하여(게이트웨이에 해당 서버 인증서가 설치되어 있어야 함) 생성한 일반 HTTP 요청을 목적지 서버로 전달한다.
    * 이런 보안 가속기는 원 서버에 고성능 암호화 기능을 제공할 목적으로 웹 서버 바로 앞단(보통 같은 구역 내)에 위치할 수 있다.
  * HTTP/CGI 서버 측 애플리케이션 게이트웨이
    * 애플리케이션 서버 게이트웨이 API를 통해 HTTP 클라이언트를 서버 측 애플리케이션 프로그램에 연결한다.
    * 웹에서 물건을 사거나 일기예보, 주식 시세를 조회할 때 실은 애플리케이션 서버 게이트웨이를 방문하는 것이다.

### 8.1.1 클라이언트 측 게이트웨이와 서버 측 게이트웨이

* 웹 게이트웨이는 한쪽에서는 HTTP로, 다른 한쪽에서는 HTTP가 아닌 다른 프로토콜로 통신한다.
  * 상이한 HTTP 버전 사이에서 변환을 수행하는 웹 프락시는 게이트웨이와 같다. 하지만 양쪽에서 HTTP로 통신하기 때문에 기술적으로는 프락시다.
* 게이트웨이는 클라이언트 측 프로토콜과 서버 측 프로토콜을 빗금(/)으로 구분해 기술한다: `<클라이언트 프로토콜>/<서버 프로토콜>`
  * 예) HTTP 클라이언트와 NNTP 뉴스 서버 사이에서는 `HTTP/NNTP` 게이트웨이
* 게이트웨이의 역할
  * 서버 측 게이트웨이는 클라이언트와 HTTP로 통신하고, 서버와는 외래 프로토콜로 통신한다.
  * 클라이언트 측 게이트웨이는 클라이언트와 외래 프로토콜로 통신하고, 서버와는 HTTP로 통신한다.

## 8.2 프로토콜 게이트웨이

* 프락시처럼 게이트웨이에도 HTTP 트래픽을 바로 보낼 수 있다.
* 브라우저에 명시적으로 게이트웨이를 설정해 자연스레 트래픽이 게이트웨이를 거쳐 가게 하거나, 게이트웨이를 대리 서버(리버스 프락시)로 설정할 수도 있다.
* 예) 특정 URL을 모든 FTP URL에 대한 HTTP/FTP 게이트웨이로 설정했다면,
  ```
  HTTP 클라이언트 ------------------ HTTP ------------------->  웹 서버

  HTTP 클라이언트 ---- HTTP ----> HTTP/FTP 게이트웨이 <-- FTP --> FTP 서버
  ```
  * 일반적인 HTTP 트래픽에는 영향을 끼치지 않는다. 일반 HTTP 트래픽은 원 서버로 바로 보낸다.
  * FTP URL을 포함한 요청은 특정 주소의 게이트웨이로 HTTP 요청을 보낸다.
  * 게이트웨이는 클라이언트 측 요청을 FTP 요청으로 반환하여 처리한 뒤 그 결과를 클라이언트에게 HTTP로 전송한다.

### 8.2.1 HTTP/*: 서버 측 웹 게이트웨이

* 서버 측 웹 게이트웨이는 클라이언트로부터 HTTP 요청이 원 서버 영역으로 들어오는 시점에 클라이언트 측 HTTP 요청을 외래 프로토콜로 전환한다.
* HTTP/FTP 게이트웨이
  * HTTP 요청을 FTP 요청으로 변환한다.
  * 게이트웨이는 원 서버의 FTP 포트로 FTP 커넥션을 연결하고 FTP 프로토콜을 통해 객체를 가져온다.
  * 게이트웨이가 하는 일
    * USER와 PASS 명령을 보내서 서버에 로그인한다.
    * 서버에서 적절한 디렉터리로 변경하기 위해 CWD 명령을 내린다.
    * 다운로드 형식을 ASCII로 설정한다.
    * MDTM으로 문서의 최근 수정 시간을 가져온다.
    * PASV로 서버에게 수동형 데이터 검색을 하겠다고 말한다.
    * RETR로 객체를 검색한다.
    * 제어 채널에서 반환된 포트로 FTP 서버에 데이터 커넥션을 맺는다. 데이터 채널이 열리는 대로, 객체가 게이트웨이로 전송된다.
  * 게이트웨이는 객체를 받는 대로 HTTP 응답에 실어 클라이언트에게 전송한다.

### 8.2.2 HTTP/HTTPS: 서버 측 보안 게이트웨이

* 기업 내부 모든 웹 요청을 암호화함으로써 개인 정보 보호와 보안을 제공하는 데 게이트웨이를 사용할 수 있다.
* 클라이언트는 일반 HTTP를 사용하여 웹을 탐색할 수 있으나 게이트웨이는 자동으로 사용자의 모든 세션을 암호화할 것이다.

### 8.2.3 HTTPS/HTTP: 클라이언트 측 보안 가속 게이트웨이

* HTTPS/HTTP 게이트웨이는 보안 가속기로 유명하다. 이는 웹 서버 앞단에 위치하고 보이지 않는 인터셉트 게이트웨이나 리버스 프락시 역할을 한다.
* 이 게이트웨이는 보안 HTTPS 트래픽을 받아서 복호화하고, 웹 서버로 보낼 일반 HTTP 요청을 만든다.
* 원 서버보다 더 효율적으로 보안 트래픽을 복호화하는 암호화 하드웨어를 내장해서 원 서버의 부하를 줄여주기도 한다.
  * 하지만 게이트웨이와 원 서버 간 암호화하지 않은 트래픽을 전송하므로, 게이트웨이와 원 서버 간에 있는 네트워크가 안전한지 확인을 확실히 하고 사용해야 한다.

## 8.3 리소스 게이트웨이

* 게이트웨이의 가장 일반적인 형태인 애플리케이션 서버는 목적지 서버와 게이트웨이를 한 개의 서버로 결합한다.
* 애플리케이션 서버는 HTTP를 통해 클라이언트와 통신하고 서버 측 애플리케이션 프로그램에 연결하는 서버 측 게이트웨이다.
* 애플리케이션 서버는 HTTP 클라이언트를 여러 백엔드 애플리케이션으로 연결한다.
  * 서버로부터 파일이 전송되는 대신, 애플리케이션 서버는 게이트웨이의 애플리케이션 프로그래밍 인터페이스(Application Programming Interface, API)를 통해서 서버에서 동작하는 애플리케이션에게 요청을 전달한다.
* 애플리케이션 게이트웨이에서 유명했던 최초의 API는 공용 게이트웨이 인터페이스(Common Gateway Interface, CGI)였다.
  * CGI는 특정 URL에 대한 HTTP 요청에 따라 프로그램을 실행하고, 프로그램 출력을 수집하고, HTTP 응답으로 회신하는데 웹 서버가 사용하는 표준화된 인터페이스 집합이다.
* 게이트웨이의 동작
  * 게이트웨이를 통해야 받을 수 있는 리소스 요청이 들어오면, 서버는 헬퍼 애플리케이션을 생성하여 요청을 처리한다.
  * 헬퍼 애플리케이션은 필요한 데이터(요청 전체 또는 사용자가 데이터베이스에서 실행시킬 질의 등)를 전달받는다.
  * 그 다음 바로 클라이언트로 전달할 응답 또는 응답 데이터를 서버에 반환한다.
  * 서버와 게이트웨이는 별개 애플리케이션이라 각각 가진 책임이 분명히 나뉜다.

### 8.3.1 공용 게이트웨이 인터페이스

* 공용 게이트웨이 인터페이스(CGI)는 최초이자 지금까지 널리 쓰이는 서버 확장이다.
  * 웹에서 동적인 HTML, 신용카드 처리, 데이터베이스 질의 등을 제공할 때 쓰인다.
* CGI 애플리케이션이 서버와 분리되면서 펄(Perl), Td, C, 다양한 셀 언어를 포함해 많은 언어로 구현할 수 있게 되었다.
* CGI는 단순하여 거의 모든 HTTP 서버가 지원한다.
* CGI가 내부에서 어떤 처리를 하는지는 사용자에게 보이지 않는다. 사용자 시각에서는 CGI가 내부적으로 일반적인 요청을 만드는 것일 뿐이다.
  * 서버와 CGI 애플리케이션 간에 진행되는 처리 단계도 감춰져 있다. URL의 'cgi' 혹은 '?'로 CGI 애플리케이션이 뭔가 하고 있다는 것을 알 수 있다.
* CGI는 거의 모든 리소스 형식과 서버의 접점에 있으면서 필요에 따라 어떤 변형이든 처리해내는 단순한 기능을 제공한다.
* 인터페이스는 문제가 많은 확장으로부터 서버를 보호한다는 점에서 훌륭하다고 할 수 있다.
* 그러나 이런 분리 때문에 성능 관련 비용이 발생한다. 모든 CGI 요청마다 새로운 프로세스를 만드는 데 부하가 꽤 크고, CGI 사용 서버의 성능을 제한하며, 서버 장비에 부담을 준다.
* 이 문제를 피하고자 새로운 CGI 형식인 Fast CGI가 개발되었다. 이는 CGI와 유사하지만 데몬으로 동작함으로써 요청마다 새로운 프로세스를 만들고 제거하면서 생기는 성능 저하 문제를 해결했다.

### 8.3.2 서버 확장 API

* CGI 프로토콜은 구동 중인 HTTP 서버에 외부 인터프리터가 쉽게 접속할 수 있게 해준다.
* 서버 자체의 동작을 바꾸고 싶거나 서버 처리 능력을 최고치로 끌어올리고자, 서버 개발자는 웹 개발자가 자신의 모듈을 HTTP와 직접 연결할 수 있는 강력한 인터페이스인 서버 확장 API를 제공한다.
* 확장 API는 프로그래머가 자신의 코드를 서버에 연결하거나 서버의 컴포넌트를 자신이 만든 것으로 교체할 수 있게 한다.
* 유명한 서버 대부분은 개발자에게 확장 API를 한 개 이상 제공한다.
* 이런 확장은 서버 자체의 아키텍처에 의존하기 때문에, 대부분 한 가지 서버 형식으로 특화되었다.
* 서버 확장의 예) 프론트페이지 제작자가 웹 출판 서비스를 하게 지원해주는 마이크로소프트의 프론트페이지 서버 확장(FrontPage Server Extension, FPSE)
  * 이는 프론트페이지 클라이언트로부터 전송되는 원격 프로시져 호출(remote procedure call, RPC) 명령을 인식할 수 있다.
  * 이 명령은 HTTP에 편승하여 온다. (특히 HTTP POST 메서드 상에 붙어서 온다) (19장 참고)

## 8.4 애플리케이션 인터페이스와 웹 서비스

* 애플리케이션을 연결하며 생기는 이슈 중 하나는 데이터를 교환하려는 두 애플리케이션 사이에서 프로토콜 인터페이스를 맞추는 일이다.
* 애플리케이션 상호 운용을 하다보면, HTTP 헤더로는 표현이 힘든 복잡한 정보를 교환하기 위해 HTTP 확장이나 HTTP 위에 프로토콜을 덧씌울 수도 있다. (19장 참고)
* 인터넷 커뮤니티는 각 애플리케이션이 서로 통신하는데 사용할 표준과 프로토콜 집합을 개발하였고, 이런 표준을 웹 서비스로 부르게 되었다. 여기서의 웹 서비스는 애플리케이션이 정보를 공유하는데 사용하는 새로운 메커니즘을 의미한다.
* 웹 서비스는 HTTP 같은 표준 웹 기술 위에서 개발한다.
* 웹 서비스는 SOAP을 통해 XML을 사용하여 정보를 교환한다.
  * XML(eXtensible Markup Language)은 데이터 객체를 담는 데이터를 생성하고 해석하는 방식을 제공한다.
  * SOAP(Simple Object Access Protocol)은 HTTP 메시지에 XML 데이터를 담는 방식에 관한 표준이다.

## 8.5 터널

* 웹 터널은 HTTP 프로토콜을 지원하지 않는 애플리케이션에 HTTP 애플리케이션을 사용해 접근하는 방법을 제공한다.
* 웹 터널을 사용하면 HTTP 커넥션을 통해 HTTP가 아닌 트래픽을 전송할 수 있고, 다른 프로토콜을 HTTP 위에 올릴 수 있다.
* 웹 터널을 사용하는 가장 일반적 이유는 HTTP 커넥션 안에 HTTP가 아닌 트래픽을 전송할 수 있다.

### 8.5.1 CONNECT로 HTTP 터널 커넥션 맺기

* 웹 터널은 HTTP의 CONNECT 메서드를 사용하여 커넥션을 맺는다.
* CONNECT 프로토콜은 HTTP/1.1 명세에 자세히 나와 있지는 않지만, 많이 구현하는 확장이다.
* CONNECT 메서드는 터널 게이트웨이가 임의의 목적 서버와 포트에 TCP 커넥션을 맺고 클라이언트와 서버 간 데이터를 무조건 전달하기를 요청한다.
* CONNECT 메서드가 게이트웨이로 터널을 연결하는 방식
  1. 클라이언트는 게이트웨이에 터널을 연결하려고 CONNECT 요청을 보낸다. 클라이언트의 CONNECT 메서드는 TCP 커넥션을 위해 게이트웨이에 터널 연결을 요청한다.
  2. 443 포트로 TCP 커넥션을 열고 커넥션이 생성된다.
  3. TCP 커넥션이 맺어지면 게이트웨이는 클라이언트에게 `HTTP 200 Connection Established` 응답을 전송하여 연결됨을 알린다.
  4. 이 시점에 터널이 연결된다. HTTP 터널을 통해 전송된 클라이언트의 모든 데이터는 위에서 맺은 TCP 커넥션으로 바로 전달되며, 서버로부터 전송된 모든 데이터 역시 HTTP 터널을 통해 클라이언트에게 전달된다.
* CONNECT 메서드는 모든 서버나 프로토콜에 TCP 커넥션을 맺는데 사용할 수 있다.

#### CONNECT 요청

* CONNECT 문법은 시작줄을 제외하고 다른 HTTP 메서드와 같다.
* 요청 URI는 호스트 명이 대신하며, 콜론에 이어 포트를 기술한다.

```
CONNECT home.netscape.com:443 HTTP/1.0
User-agent: Mozilla/4.0
```

* 시작줄 다음에는 다른 HTTP 메시지와 같이 추가적인 HTTP 요청 헤더 필드가 올 수 있다.
* 보통 각 행은 CRLF로 끝나고, 헤더 목록의 끝은 빈 줄의 CRLF로 끝난다.

#### CONNECT 응답

* 클라이언트는 요청을 전송한 다음, 게이트웨이의 응답을 기다린다.
* 일반 HTTP 메시지처럼 200 응답 코드는 성공을 뜻한다. 편의상 응답의 사유 구절은 `Connection Established`로 기술된다.
  ```
  HTTP/1.0 200 Connection Established
  Proxy-agent: Netscape-Proxy/1.1
  ```
* 일반적 HTTP 응답과는 달리 `Content-Type` 헤더를 포함할 필요는 없다. 커넥션이 메시지 전달 대신 바이트를 그대로 전달하기 때문에 콘텐츠 형식을 기술할 필요가 없다.

### 8.5.2 데이터 터널링, 시간, 커넥션 관리

* 터널을 통해 전달되는 데이터는 게이트웨이에서 볼 수 없어서 게이트웨이는 패킷의 순서나 흐름에 대한 어떤 가정도 할 수 없다. 터널이 일단 연결되면, 데이터는 언제 어디로든 흘러가버릴 수 있다.
* 클라이언트는 성능을 높이기 위해 CONNECT 요청을 보낸 다음, 응답을 받기 전에 터널 데이터를 전송할 수 있다.
  * 이는 서버에 데이터를 더 빨리 보내는 방법이나, 게이트웨이가 요청에 이어 데이터를 적절하게 처리할 수 있어야 함을 전제로 한다.
  * 특히 게이트웨이는 네트워크 I/O 요청이 헤더 데이터만 반환해줄 거라 가정할 수 없어서, 커넥션이 맺어지는 대로 헤더를 포함해서 읽어들인 모든 데이터를 서버에 전송해야 한다.
  * 요청 후 터널을 통해 데이터를 전송한 클라이언트는 인증 요구(authentication challenge)나 200 외 응답이 왔을 때 요청 데이터를 다시 보낼 준비가 되어 있어야 한다.
* 터널의 끝단 어느 부분이든 커넥션이 끊어지면, 그 끊어진 곳으로부터 온 데이터는 반대편으로 전달된다. 그 다음 커넥션이 끊어졌던 터널 끝단 반대편의 커넥션도 프락시에 의해 끊어진다. 커넥션이 끊긴 한쪽에 아직 전송하지 않은 데이터는 버려진다.

### 8.5.3 SSL 터널링

* 웹 터널은 원래 방화벽을 통해 암호화된 SSL 트래픽을 전달하려고 개발되었다.
* SSL과 같이 암호화된 프로토콜은 정보가 암호화되어 있어서 낡은 방식의 프락시에서는 처리되지 않는다.
* 터널을 사용하면 SSL 트래픽을 HTTP 커넥션으로 전공하여 80 포트의 HTTP만 허용하는 방화벽을 통과시킬 수 있다.
* SSL 트래픽이 기존 프락시 방화벽을 통과할 수 있게 HTTP에 터널링 기능이 추가되었다. 이 기능은 HTTP 메시지에 암호화된 날 데이터를 담고 일반 HTTP 채널을 통해 데이터를 전송한다.
* 직접 SSL 커넥션에서는 보안 웹 서버로 SSL 트래픽이 바로 전송된다.(SSL 포트인 443으로)
* HTTP 터널을 통한 SSL에서, SSL 트래픽은 일반 SSL 커넥션을 통해 전송되기 전까지는 HTTP 메시지에 담겨 HTTP 포트인 80에 전송된다.
* 터널은 HTTP가 아닌 트래픽이 포트를 제한하는 방화벽을 통과할 수 있게 해준다. 이는 보안 SSL 트래픽이 방화벽을 통과하는 데 유용하게 사용될 수 있다. 그러나 터널은 악의적인 트래픽이 사내로 유입되는 경로가 될 수 있다.

### 8.5.4 SSL 터널링 vs HTTP/HTTPS 게이트웨이

* HTTPS 프로토콜(SSL 상의 HTTP)은 다른 프로토콜과 같은 방식으로 게이트웨이를 통과할 수 있다.
* 원격 HTTPS 서버와 SSL 세션을 시작하는 게이트웨이(클라이언트 대신)를 두고 클라이언트 측의 HTTPS 트랜잭션을 수행하는 방식이다.
* 응답은 프락시가 받아서 복호화하고 난 후, HTTP를 통해 클라이언트로 전송한다. 이는 게이트웨이가 FTP를 처리하는 방식과 같다.
  * 이 접근의 단점
    * 클라이언트-게이트웨이 사이에는 보안이 적용되지 않은 일반 HTTP 커넥션이 맺어져 있다.
    * 프락시가 인증을 담당하고 있기 때문에, 클라이언트는 원격 서버에 SSL 클라이언트 인증(X509 인증서 기반의 인증)을 할 수 없다.
    * 게이트웨이는 SSL을 완벽히 지원해야 한다.
* 이 상황에서 SSL 터널링을 사용하면 프락시에 SSL을 구현할 필요가 없다.
* SSL 세션은 클라이언트가 생성한 요청과 목적지(보안이 적용된) 웹 서버 간에 생성된다.
* 프락시 서버는 트랜잭션의 보안에 관여하지 않고 암호화된 데이터를 그대로 터널링한다.

### 8.5.5 터널 인증

* HTTP의 다른 기능들을 터널과 함께 사용할 수 있다.
* 특히 프락시 인증 기능은 클라이언트가 터널을 사용할 수 있는 권한을 검사하는 용도로 터널에서 사용할 수 있다.
* 게이트웨이는 터널 사용 허가를 내리기 전 프락시 인증을 할 수 있다.
  1. 터널은 클라이언트와 게이트웨이 사이에 놓인다.
  2. 클라이언트가 CONNECT 요청을 전송한다.
  3. 게이트웨이가 인증 요구를 반환하고 클라이언트는 적절한 인증과 함께 CONNECT 요청을 전송한다.
  4. 게이트웨이는 443 포트로 TCP 커넥션을 열고, 커넥션이 맺어지면 HTTP 커넥션 준비 메시지를 클라이언트에게 반환한다.

### 8.5.6 터널 보안에 대한 고려사항들

* 보통 터널 게이트웨이는 통신하고 있는 프로토콜이 터널을 올바른 용도로 사용하고 있는지 검증할 방법이 없다.
* 예) 회사 직원이 게임을 하기 위해 회사 방화벽에 터널을 생성하여 게임 트래픽을 사내로 유입시킬 수도 있고, 악의적 사용자가 회사에 텔넷 세션을 열거나 회사 이메일 차단 장치를 우회하려고 터널을 사용할 수 있다.
* 터널 오용을 최소화하기 위해, 게이트웨이는 HTTPS 전용 포트인 443 같이 잘 알려진 특정 포트만 터널링할 수 있게 허용해야 한다.

## 8.6 릴레이

* HTTP 릴레이는 HTTP 명세를 완전히 준수하지 않는 간단한 HTTP 프락시다.
* 릴레이는 커넥션을 맺기 위한 HTTP 통신을 한 후, 바이트를 맹목적으로 전달한다.
* HTTP는 복잡해서 모든 헤더와 메서드 로직을 수행하지 않고 맹목적으로 트래픽을 전달하는 간단한 프락시 구현 방식이 유용할 때가 있다.
* 데이터를 맹목적으로 전달하도록 구현하기는 쉬워서 단순 필터링이나 진단, 콘텐츠 변환을 하는데 사용되기도 한다. 그러나 이는 잠재적으로 심각한 상호 운용 문제를 가지고 있기 때문에 주의해서 배포해야 한다.
* 단순 맹목적 릴레이를 구현하는데 관련된 더 일반적인 문제 중 하나는, 맹목적 릴레이가 `Connection` 헤더를 제대로 처리하지 못해서 keep-alive 커넥션이 행(hang)에 걸리는 것이다.
  1. 웹 클라이언트는 `Connection: Keep-Alive` 헤더를 보내서, 릴레이에 keep-alive 커넥션을 맺기 희망한다는 요청 메시지를 전송한다. 클라이언트는 keep-alive 채널에 대한 요청이 받아들여졌는지 확인하기 위해 응답을 기다린다.
  2. 릴레이가 HTTP 요청을 받지만, `Connection` 헤더를 이해하지 못하므로, 단순히 요청을 서버로 넘긴다. `Connection` 헤더는 홉과 홉 사이(hop-by-hop)에만 사용하는 헤더다. 이는 단일 전송 링크만 지원하고 체인을 따라 전달할 수 없다. 문제는 여기서 시작된다.
  3. 릴레이 된 HTTP 요청이 웹 서버에 도착한다. 웹 서버가 프락시로부터 `Connection: Keep-Alive` 헤더를 받으면, keep-alive 요청을 받은 것으로 생각하고 `Connection: Keep-Alive` 응답 헤더로 응답한다. 이 시점부터 웹 서버는 릴레이와 함께 keep-alive 통신을 하고, keep-alive 규칙에 맞게 동작할 것이다. 하지만 릴레이는 keep-alive에 대해 아무것도 모른다.
  4. 릴레이는 웹 서버로부터 받은 `Connection: Keep-Alive` 헤더를 포함한 응답 메시지를 클라이언트에게 전달한다. 클라이언트는 이 헤더를 통해 릴레이가 keep-alive로 통신하는 것에 동의했다고 추측한다. 이 시점에 클라이언트와 서버는 keep-alive로 통신하고 있다고 믿지만, 실제로 통시하는 릴레이는 keep-alive가 무엇인지 모른다.
  5. 릴레이는 keep-alive에 대해 아무것도 모르기 때문에 원 서버가 커넥션을 끊기를 기다리며 받은 데이터 전부를 그대로 클라이언트에게 보낸다. 원 서버는 릴레이가 자신에게 커넥션을 계속 맺고 있기를 요청했다고 믿고 커넥션을 끊지 않을 것이다. 릴레이는 커넥션이 끊길 때를 기다리며 계속 커넥션을 맺고(hang) 있는다.
  6. 클라이언트가 응답 메시지를 받으면, 바로 다음 요청을 keep-alive 커넥션을 통해 릴레이에게 전송한다. 단순 릴레이는 같은 커넥션으로 또 다른 요청이 오는 것을 예측하지 못한다. 브라우저는 계속 돌지만 아무 작업도 진행되지 않는다.
* 이런 위험 예방을 위해 릴레이를 좀 더 똑똑하게 만들 수도 있으나, 프락시의 단순함 이면에는 상호 운용과 관련한 문제가 발생할 위험이 있다.
* 특정 목적을 위해 단순한 HTTP 릴레이를 구축할 때는 어떻게 사용할지 신중히 고민해야 한다. 여러 문제 예방을 위해 HTTP를 제대로 준수하는 프락시를 사용하는 게 좋다.

# 9. 웹 로봇

* 웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션을 자동으로 수행하는 소프트웨어 프로그램이다.
* 많은 로봇이 웹 사이트에서 다른 웹 사이트로 떠돌아다니며 콘텐츠를 가져오고, 발견한 데이터를 처리한다.
* 이런 로봇은 자동으로 웹 사이트를 탐색하며 그 방식에 따라 '크롤러', '스파이더', '웜', '봇' 등 각양각색의 이름으로 불린다.
* 웹 로봇의 예
  * 주식시장 서버에 매 분 HTTP GET 요청을 보내고, 여기서 얻은 데이터를 활용해 주가 추이 그래프를 생성하는 주식 그래프 로봇
  * 월드 와이드 웹의 규모와 진화에 대한 통계 정보를 수집하는 웹 통계 조사 로봇
    * 웹을 떠돌며 페이지의 개수를 세고, 각 페이지의 크기, 언어, 미디어 타입을 기록한다. [netcraft.com](http://www.netcraft.com)
  * 검색 데이터베이스를 만들기 위해 발견한 모든 문서를 수집하는 검색엔진 로봇
  * 상품 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰 카탈로그에서 웹페이지를 수집하는 가격 비교 로봇

## 9.1 크롤러와 크롤링

* 웹 크롤러는 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇이다.
  * 웹 링크를 재귀적으로 따라가는 로봇을 크롤러 또는 스파이더라고 부른다. HTML 하이퍼링크로 만들어진 웹을 따라 '기어다니기(crawl)' 때문이다.
* 인터넷 검색엔진은 웹을 돌아다니며 만나는 모든 문서를 끌어오기 위해 크롤러를 사용한다.
  * 이 문서들은 나중에 처리되어 검색 가능한 데이터베이스로 만들어져, 사용자들이 특정 단어를 포함한 문서를 찾을 수 있게 한다.
  * 찾아서 가져와야 하는 페이지가 수십억 개나 있다 보니, 검색엔진 스파이더들은 가장 복잡한 로봇 중 하나다.

### 9.1.1 어디에서 시작하는가: '루트 집합'

* 크롤러가 동작하기 전 우선 출발지점을 주어야 한다. 크롤러가 방문을 시작하는 URL들의 초기 집합을 루트 집합(root set)이라고 한다.
* 루트 집합을 고를 때, 모든 링크를 크롤링하면 결과적으로 관심 있는 페이지 대부분을 가져올 수 있도록 충분히 다른 장소에서 URL들을 선택해야 한다.
  * 진짜 웹에서는 최종적으로 모든 문서로 이어지게 되는 하나의 문서란 없다.
  * 일반적으로 웹 대부분을 커버하기 위해 루트 집합에 너무 많은 페이지가 있을 필요는 없다.
* 보통 좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 자주 링크되지 않은 잘 알려지지 않은 페이지 목록으로 구성되어 있다.
* 인터넷 검색엔진에서 쓰이는 것과 같은 많은 대규모 크롤러 제품은 사용자들이 루트 집합에 새 페이지, 잘 알려지지 않은 페이지를 추가하는 기능을 제공한다.
* 이 루트 집합은 시간이 지남에 따라 성장하며 새 크롤링을 위한 시드 목록이 된다.

### 9.1.2 링크 추출과 상대 링크 정상화

* 크롤러는 검색한 각 페이지 안에 들어있는 URL 링크를 파싱하여 크롤링할 페이지 목록에 추가한다.
* 크롤링을 진행하며 탐색할 새 링크를 발견함에 따라 이 목록은 보통 급속히 확장된다.
* 크롤러는 간단한 HTML 파싱을 해서 링크를 추출하고 상대 링크를 절대 링크로 변환한다.

### 9.1.3 순환 피하기

* 로봇에 웹을 크롤링할 때 루프나 순환에 빠지지 않도록 매우 조심해야 한다.
* 로봇은 순환을 피하기 위해 반드시 어디를 방문했는지 알아야 한다. 순환은 로봇을 함정에 빠뜨려 멈추게 하거나 진행을 느려지게 한다.

### 9.1.4 루프와 중복

* 순환은 최소한 다음과 같은 이유로 크롤러에게 해롭다.
  * 순환은 크롤러를 루프에 빠뜨려 꼼짝 못하게 만들 수 있다. 루프는 허술하게 설계된 크롤러를 빙빙 돌게 할 수 있고, 같은 페이지를 반복해서 가져오는데 시간을 허비하게 만들 수 있다. 이런 크롤러가 네트워크 대역폭을 다 차지하여 다른 어떤 페이지도 가져올 수 없게 될 수 있다.
  * 크롤러가 같은 페이지를 반복해서 가져오면 고스란히 웹 서버의 부담이 된다. 크롤러의 네트워크 접근 속도가 충분히 빠르면, 웹 사이트를 압박하여 실제 사용자들이 사이트에 접근할 수 없게 막을 수도 있다. 이런 서비스 방해 행위는 법적 문제제기의 근거가 될 수 있다.
  * 루프 자체가 문제되지 않더라도, 크롤러는 수 많은 중복 페이지(dups)를 가져와서 쓸모없게 된다.

### 9.1.5 빵 부스러기의 흔적

* 방문한 곳을 지속적으로 추적하는 것은 쉽지 않다.
* 전 세계 웹 콘텐츠 상당 부분을 크롤링하려면 수십억 개의 URL을 방문해야 하고, 어떤 곳을 방문했는지 빠르게 판단하기 위해 복잡한 자료 구조를 사용할 필요가 있다.
  * 이 자료구조는 속도, 메모리 사용 면에서 효과적이어야 한다.
  * 로봇은 어떤 URL이 방문한 곳인지 빠른 결정을 위해 적어도 검색 트리나 해시 테이블을 필요로 할 것이다.
* 수억 개의 URL은 많은 공간을 차지한다. 평균 URL이 40바이트 길이이고, 웹 로봇이 5억 개의 URL을 크롤링 했다면, 검색 데이터 구조는 이 URL을 유지하기 위해 20GB 이상 메모리가 필요하다.

대규모 웹 크롤러가 방문한 곳 관리를 위해 사용하는 유용한 기법을 알아보자.

##### 트리와 해시 테이블
* 복잡한 로봇이 방문한 URL 추적을 위해 트리와 해시 테이블을 사용할 수 있다. 이는 URL을 훨씬 더 빨리 찾아볼 수 있게 하는 소프트웨어 자료 구조다.

##### 느슨한 존재 비트맵
* 공간 사용을 최소화하기 위해, 몇몇 대규모 크롤러는 존재 비트 배열(presence bit array)과 같은 느슨한 자료 구조를 사용한다.
* 각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는 '존재 비트(presence bit)'를 갖는다.
* URL이 크롤링 되었을 때, 해당하는 존재 비트가 만들어진다. 존재 비트가 이미 존재한다면 그 URL은 이미 크롤링 되었다고 간주한다.

##### 체크포인트

* 로봇 프로그램이 갑작스레 중단될 경우를 대비해, 방문한 URL 목록이 디스크에 저장되었는지 확인한다.

##### 파티셔닝

* 웹이 성장하면서 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것이 불가능해졌다.
  * 크롤링을 완수하기에 한 대의 컴퓨터로는 메모리, 디스크 공간, 연산 능력, 네트워크 대역폭이 충분치 않을 수 있다.
* 몇몇 대규모 웹 로봇은 각각이 분리된 한 대의 컴퓨터인 로봇들이 동시에 일하는 '농장(farm)'을 이용한다.
  * 각 로봇에 URL들의 특정 '한 부분'이 할당되어 그에 대한 책임을 지고 서로 도와 크롤링 한다.
  * 개별 로봇은 URL을 이리저리 넘겨주거나 오작동하는 동료를 돕거나, 그 외 이유로 활동을 조정하기 위한 커뮤니케이션을 할 수 있다.
* 거대 자료 구조 구현을 위한 참고 도서: [Managing Gigabytes: Compressing and indexing documents and images(1999)](https://www.amazon.com/Managing-Gigabytes-Compressing-Multimedia-Information/dp/1558605703)

### 9.1.6 별칭(alias)과 로봇 순환

* 올바른 자료 구조를 갖췄어도 URL이 별칭을 가질 수 있기 때문에 방문 여부를 체크하는게 쉽지 않을 수 있다.
* 한 URL이 또 다른 URL의 별칭이라면 이 둘이 서로 달라보여도 사실을 같은 리소스를 가리키고 있다.
* 같은 문서를 가리키는 다른 URL
  * 기본 포트 번호 80이 붙었을 때
  * `%7`이 `~`과 같을 때(이스케이핑 문자)
  * `#` 태그에 따라 페이지가 바뀌지 않을 때
  * 서버가 대소문자를 구분하지 않을 때
  * 기본 페이지가 index.html일 때
  * 호스트 명 대신 아이피 주소를 가질 때

### 9.1.7 URL 정규화하기

* 대부분 웹 로봇은 URL을 표준 형식으로 '정규화'하여 다른 URL과 같은 리소스를 가리키고 있는 확실한 것을 미리 제거하려고 한다.
  1. 포트 번호가 명시되지 않았다면, 호스트 명에 `:80`을 추가한다.
  2. 모든 `%xx` 이스케이핑 문자를 대응되는 문자로 변환한다.
  3. `#` 태그를 제거한다.
* 그 밖의 중복 문제 해결을 위해서는 웹 서버에 대한 지식이 필요하다.
  * 로봇은 웹 서버가 대소문자 구분을 하는지 알아야 한다.
  * 디렉터리에 대한 웹 서버의 색인 페이지 설정을 알아야 한다.
  * URL의 호스트 명과 IP 주소가 같은 물리적 컴퓨터를 참조한다는 것과, 웹 서버가 가상 호스팅을 하도록 설정되어 있는지 알아야 한다.
* URL 정규화는 기본적 문법의 별칭을 제거할 수 있으나, 로봇들은 URL을 표준 형식으로 변환하는 것 만으로 제거할 수 없는 다른 URL 별칭도 만나게 될 것이다.

### 9.1.8 파일 시스템 링크 순환

* 파일 시스템의 심벌릭 링크는 사실상 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉터리 계층을 만들 수 있기 때문에, 매우 교묘한 순환을 유발할 수 있다.
* 심벌릭 링크 순환은 보통 서버 관리자의 실수로 만들게 되지만, 때로 사악한 웹 마스터가 악의적으로 만들기도 한다.
  * 파일 시스템에서 `subdir/`이 상위 `/`를 가리키는 심벌릭 링크일 경우, `index.html`에서 이어지는 `subdir/index.html`을 가져와도 같은 `index.html`로 되돌아오며 순환된다.
  * URL이 달라 보이기 때문에 URL만으로는 문서가 같은 것을 모르고 루프에 빠질 수 있다.

### 9.1.9 동적 가상 웹 공간

* 악의적으로 복잡한 크롤러 루프를 만드는 경우도 있을 수 있다. 특히 평범한 파일처럼 보이지만 게이트웨이 애플리케이션인 URL을 만들기 쉽다.
* 이 애플리케이션은 같은 서버에 있는 가상 URL에 대한 링크를 포함한 HTML을 즉석에서 만들어낼 수 있다. 이 가상 URL로 요청을 받으면 새로운 가상 URL을 갖고 있는 새 HTML 페이지를 날조하여 만들어낸다.
* 악의적인 웹 서버는 실제로 파일을 하나도 갖고 있지 않으면서도 가상 웹 공간 너머로 로봇을 무한히 떠돌게 할 수 있다. URL과 HTML은 매번 전혀 달라보일 수 있기 때문에 로봇이 순환을 감지하기 매우 어렵다.
* 이보다 흔하게, 웹 마스터가 악의 없이 자신도 모르게 심벌릭 링크나 동적 콘텐츠를 통한 크롤러 함정을 만들 수 있다.
  * ex) 한 달 치 달력을 생성하고 그 다음 달로 링크를 걸어주는 CGI 기반 달력 프로그램: 콘텐츠의 동적 성질을 이해하지 못하는 로봇은 무한히 다음 달 달력을 요청할 수 있다.

### 9.1.10 루프와 중복 피하기

* 모든 순환을 피하는 완벽한 방법은 없다. 실제로 잘 설계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 한다.
* 이들 휴리스틱은 문제를 피하는데 도움을 주지만 동시에 약간 '손실'을 유발할 수도 있다. 의심스러워 보이지만 실은 유효한 콘텐츠를 거르게 될 수도 있기 때문이다.
* 웹에서 로봇이 문제를 일으킬 가능성이 크다. 이러한 웹에서 로봇이 더 올바르게 동작하기 위해 사용하는 기법이 있다.

##### URL 정규화

* URL을 표준 형태로 변환함으로써 같은 리소스를 가리키는 중복 URL이 생기는 것을 일부 회피한다.

##### 너비 우선 크롤링

* 크롤러는 언제든 크롤링할 수 있는 URL의 큰 집합을 갖고 있다.
* 방문할 URL을 웹 사이트 전체에 걸쳐 너비 우선으로 스케쥴링하면, 순환의 영향을 최소화할 수 있다.
* 혹 로봇 함정을 건드리더라도, 여전히 그 순환에서 페이지를 받아오기 전에 다른 웹 사이트들에서 수십만 개의 페이지를 받아올 수 있다.
* 로봇을 깊이 우선 방식으로 운용하여 웹 사이트 하나에 성급히 뛰어들면, 순환을 건드리는 경우 영원이 다른 사이트로 빠져나올 수 없게 될 것이다.
* 너비 우선 크롤링은 요청을 더 분산시켜 특정 서버가 압박 받지 않도록 해주며, 한 로봇이 한 서버의 자원을 최소로 사용하도록 유지해준다.

##### 스로틀링

* 로봇이 웹 사이트에서 일정 시간동안 가져올 수 있는 페이지 숫자를 제한한다.
* 로봇이 순환을 건드려 지속적으로 그 사이트 별칭에 접근을 시도하면, 스로틀링을 이용해 그 서버에 대한 접근 횟수와 중복의 총 횟수를 제한할 수 있다.

##### URL 크기 제한

* 로봇은 일정 길이(보통 1KB)를 넘는 URL의 크롤링은 거부할 수 있다. 순환으로 인해 URL이 계속해서 길어진다면 길이 제한으로 순환이 중단될 수 있다.
* 어떤 웹 서버들은 긴 URL이 주어진 경우 실패하고 URL이 점점 길어지는 순환에 빠진 로봇은 이런 웹 서버와 충돌을 유발할 수 있다. 이는 웹 마스터가 로봇을 서비스 거부 공격자로 오해하게 만든다.
* 이 기법으로 가져오지 못하는 콘텐츠가 있을 수 있음을 주의해야 한다.
  * 오늘날 많은 사이트가 URL을 사용자 상태 관리에 사용한다. (사용자 아이디를 페이지에서 참조하는 URL에 저장한다)
* URL 길이는 크롤링을 제한하는 방법으로는 까다로울 수 있으나 요청 URL이 특정 크기에 도달할 때마다 에러 로그를 남김으로써, 특정 사이트에서 어떤 일이 벌어지는지 감시하는 사용자에게 유용한 신호를 제공한다.

##### URL/사이트 블랙리스트

* 로봇 순환을 만들거나 함정으로 알려진 사이트와 URL 목록을 만들어 관리하고 피한다.
* 문제를 일으키는 사이트, URL을 발견할 때마다 이 블랙리스트에 추가한다.
* 이는 사람의 손을 필요로 한다. 그러나 오늘날의 대부분 대규모 크롤러는 블랙리스트를 가지고 있다.
* 블랙리스트는 크롤링되는 것을 싫어하는 특정 사이트를 피하기 위해 사용될 수 있다.

##### 패턴 발견

* 파일 시스템의 심벌릭 링크를 통한 순환과 그와 비슷한 오설정은 일정 패턴을 따르는 경향이 있다.
* 예를 들어 URL은 중복된 구성 요소와 함께 점점 길어질 수 있는데, 몇몇 로봇은 반복되는 구성 요소를 가진 URL을 잠재적 순환으로 보고 크롤링을 거절한다.
* 반복이 단순하지 않고 주기의 길이가 2 이상인 것도 있다. 몇몇 로봇은 다른 주기의 반복 패턴을 감지해낸다.

##### 콘텐츠 지문(fingerprint)

* 지문은 더 복잡한 웹 크롤러들이 중복을 감지하는 보다 직접적인 방법이다.
* 콘텐츠 지문을 사용하는 로봇들은 페이지 콘텐츠에서 몇 바이트를 얻어내 체크섬(checksum)을 계산한다. 이 체크섬은 그 페이지 내용의 간략한 표현이다.
* 로봇이 이전에 봤던 체크섬을 가진 페이지를 가져오면, 그 페이지의 링크는 크롤링하지 않는다. 이미 크롤링이 시작된 상태이기 때문이다.
* 체크섬 함수는 어떤 두 페이지가 서로 내용이 다름에도 체크섬은 똑같은 확률이 적은 것을 사용해야 한다. 지문 생성용으로 MD5와 같은 메시지 요약 함수가 인기 있다.
* 어떤 웹 서버는 동적으로 그때그때 페이지를 수정하므로, 로봇은 때로 웹페이지 콘텐츠에 임베딩된 링크와 같은 특정 부분을 체크섬 계산에서 빠뜨린다.
* 뿐만 아니라 페이지 콘텐츠를 임의로 커스터마이징하는 것(날짜 추가, 카운터 접근 등)을 포함한 서버 측 동적인 동작은 중복 감지를 방해할 수 있다.

##### 사람의 모니터링

* 로봇이 어떤 기법으로도 해결할 수 없는 문제에 봉착하게 될 것이다.
* 사람이 쉽게 로봇 진행 상황을 모니터링해서 문제가 일어나면 즉각 인지할 수 있게끔 반드시 진단과 로깅을 포함하여 로봇을 설계해야 한다.


* 웹 처럼 거대한 데이터 집합을 크롤링하기 위한 좋은 스파이더 휴리스틱을 만드는 작업은 언제나 현재진행형이며, 진화한다.
* 더 작고 더 커스터마이징된 크롤러는 그들이 어떤 자원에 얼마나 영향을 줄 것인지 스스로 제어할 수 있거나, 혹은 그 자원 자체가 크롤링을 수행하는 사람의 제어하에 있을 수 있기 때문에 이 문제 일부를 피해갈 수 있다. 이 크롤러는 문제 예방을 위해 사람의 모니터링에 더욱 의존한다.

## 9.2 로봇의 HTTP

* 로봇도 다른 HTTP 클라이언트 프로그램처럼 HTTP 명세 규칙을 지켜야 한다.
* HTTP 요청을 만들고 스스로를 HTTP/1.1 클라이언트라고 광고하는 로봇은 적절한 HTTP 요청 헤더를 사용해야 한다.
* 많은 로봇이 콘텐츠 요청을 위해 필요한 HTTP를 최소한으로만 구현하려 한다. 이는 문제를 유발할 수 있지만 이런 행태가 빨리 바뀔 것 같지는 않다.
  * 결과적으로 많은 로봇이 요구사항이 적은 HTTP/1.0 요청을 보낸다.

### 9.2.1 요청 헤더 식별하기

* 로봇들이 HTTP를 최소한도로만 지원하려 함에도 불구하고, 대부분 약간의 신원 식별 헤더(특히 User-Agent HTTP 헤더)를 구현하고 전송한다.
* 로봇 구현자들은 로봇의 능력, 신원, 출신을 알려주는 기본적인 몇 헤더를 사이트에게 보내주는 것이 좋다.
* 이는 잘못된 크롤러의 소유자를 찾아낼 때와 서버에게 로봇이 어떤 종류의 콘텐츠를 다룰 수 있는지에 대한 약간의 정보를 주려 할 때 유용하다.
  * User-Agent: 서버에게 요청을 만든 로봇의 이름을 말해준다.
  * From: 로봇의 사용자/관리자의 이메일 주소를 제공한다.
  * Accept: 서버에게 어떤 미디어 타입을 보내도 되는지 말해준다. 로봇이 관심있는 유형의 콘텐츠만 받게 될 것임을 확신하는데 도움을 준다.
  * Referer: 현재 요청 URL을 포함한 문서의 URL을 제공한다.

### 9.2.2 가상 호스팅

* 로봇 구현자들은 Host 헤더를 지원할 필요가 있다. 가상 호스팅이 널리 퍼진 현실에서 요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 콘텐츠를 찾게 만든다. 이런 이유로 HTTP/1.1은 Host 헤더를 사용할 것을 요구한다.
* 대부분 서버는 기본적으로 특정 사이트 하나를 운영하도록 설정되어 있다. 따라서 Host 헤더를 포함하지 않은 크롤러는 a, b 두 개의 사이트를 운영하는 서버에 b에 대한 요청을 보낼 수 있다.
  * 서버가 a를 기본으로 사용하도록 설정되어 있다면(그리고 Host 헤더를 요구하지 않는다면) 크롤러가 a 사이트 콘텐츠를 얻게 하고, 더 나쁘게는 a 콘텐츠를 b 사이트에서 온 것이라고 간주할 수 있다.

### 9.2.3 조건부 요청

* 수십억 개의 웹페이지를 다운받게 될 수도 있는 인터넷 검색 엔진 로봇과 같은 경우, 오직 변경되었을 때만 콘텐츠를 가져오도록 하는 것은 의미 있다.
* 이 로봇 중 몇몇은 시간이나 엔터티 태그를 비교하여 그들이 받아간 마지막 버전 이후 업데이트된 것이 있는지 알아보는 조건부 HTTP 요청을 구현한다.
* 이는 HTTP 캐시가 전에 받아온 리소스의 로컬 사본이 유효성을 검사하는 방법과 매우 비슷하다.

### 9.2.4 응답 다루기

* 대다수 로봇의 주 관심사는 단순히 GET 메서드로 콘텐츠를 요청해서 가져오는 것이기 때문에 응답 다루기라 부를 만한 일은 거의 하지 않는다.
* 그러나 HTTP의 몇몇 기능(조건부 요청과 같은)을 사용하는 로봇들이나, 웹 탐색이나 서버와의 상호작용을 더 잘해보려 하는 로봇은 여러 종류의 HTTP 응답을 다룰 수 있어야 한다.

##### 상태 코드

* 최소한의 일반적이고 예상할 수 있는 상태 코드를 다룰 수 있어야 한다.
* 모든 로봇은 `200 OK`나 `404 Not Found` 같은 HTTP 상태 코드를 이해해야 한다. 명시적으로 이해할 수 없는 상태 코드는 상태 코드가 속한 분류에 근거해 다뤄야 한다.
* 모든 서버가 언제나 항상 적절한 에러 코드를 반환하지 않는다는 것을 알아야 한다. 몇 서버는 에러를 기술하는 메시지도 `200 OK`로 응답하기도 한다.

##### 엔터티

* HTTP 헤더에 임베딩된 정보를 따라 로봇은 엔터티 자체의 정보를 찾을 수 있다.
* 메타 http-equiv 태그와 같은 메타 HTML 태그는 리소스에 대해 콘텐츠 저자가 포함시킨 정보다.
  * 콘텐츠 저자는 http-equiv 태그를 이용해 콘텐츠를 다루는 서버가 제공할 수 있는 헤더를 덮어쓰기할 수 있다.
    ```
    <meta http-equiv="Refresh" content="1; URL=index.html">
    ```
  * 이 태그는 수신자가 그 문서의 HTTP 응답 값이 `1; URL=index.html`인 Refresh HTTP 헤더를 포함한 것처럼 다루게 한다.
* 몇 서버는 HTML 페이지를 보내기 전 그 내용을 파싱하여 http-equiv 태그를 헤더로 포함시키지만 어떤 서버는 그렇게 하지 않는다.
* 로봇 구현자들은 http-equiv 정보를 찾아내기 위해 HTML 문서의 HEAD 태그를 탐색해야 할 수 있다.

### 9.2.5 User-Agent 타기팅

* 웹 관리자는 로봇들의 방문과 요청을 예상해야 한다.
* 많은 웹 사이트는 그들의 여러 기능을 지원하도록 브라우저 종류를 감지해 그에 맞게 콘텐츠를 최적화한다. 이로써 사이트는 로봇에게 콘텐츠 대신 에러 페이지를 제공한다.
  * "당신의 브라우저는 프레임을 지원하지 않습니다.(your browser does not support frames)"
* 로봇의 요청을 다루기 위해, 특정 브라우저의 기능이 지원되는 것을 전제로 콘텐츠를 개발하는 대신, 풍부한 기능을 갖추지 못한 브라우저나 로봇 등 다양한 클라이언트에 대응하는 유연한 웹페이지를 개발할 수 있다.
* 최소한 로봇이 사이트에 방문했다가 콘텐츠를 얻지 못하는 일이 없도록 대비해야 한다.

## 9.3 부적절하게 동작하는 로봇들

로봇이 저지르는 실수와 그로 인해 초래되는 결과

##### 폭주하는 로봇

* 로봇은 웹 서핑을 하는 사람보다 훨씬 빠르게 HTTP 요청을 만들 수 있으며, 빠른 네트워크 연결을 갖춘 빠른 컴퓨터 위에 동작할 수 있다.
* 로봇이 논리적 에러를 갖고 있거나 순환에 빠졌다면 웹 서버에 과부하를 유발할 수 있으며, 다른 누구에게도 서비스를 못하게 만들 수 있다.
* 로봇의 폭주 방지를 위한 보호 장치를 반드시 신경 써서 설계해야 한다.

##### 오래된 URL

* 로봇이 방문하는 URL의 목록이 오래되었을 수 있다. 웹 사이트가 콘텐츠를 많이 바꾸었다면, 로봇들은 존재하지 않는 URL에 대한 요청을 많이 보낼 수 있다.
* 존재하지 않는 문서에 대한 접근 요청으로 에러 로그가 채워지거나, 에러 페이지를 제공하는 부하로 인해 웹 서버의 요청에 대한 수용 능력이 감소될 수 있다.

##### 길고 잘못된 URL

* 순환이나 프로그래밍상 오류로 로봇은 웹 사이트에게 크고 의미 없는 URL을 요청할 수 있다.
* URL이 너무 크면 웹 서버의 처리 능력에 영향을 주고, 웹 서버의 접근 로그를 어지럽게 채울 수 있으며, 허술한 웹 서버라면 고장을 일으킬 수 있다.

##### 호기심이 지나친 로봇

* 어떤 로봇은 사적인 데이터의 URL을 얻어 검색엔진이나 기타 애플리케이션을 통해 쉽게 접근할 수 있게 만들어 사생활 침해를 일으킬 수 있다.
* 보통 이는 사적인 콘텐츠에 대해 이미 존재하는 하이퍼링크를 로봇이 따라감으로써 벌어지는 일이다: 소유자가 비밀이라고 생각했으나 실은 그렇지 않은 콘텐츠, 소유자가 하이퍼링크 제거를 깜박한 경우
* 이는 로봇이 명시적으로 하이퍼링크가 존재하지도 않은 문서들을 디렉터리의 콘텐츠를 가져오는 등의 방법으로 긁어올 때 일어날 수 있다.
* 민감한 데이터를 무시하는, 검색 색인이나 아카이브에서 제거하는 메커니즘은 중요하다.
* 구글과 같은 몇몇 검색엔진은 그들이 크롤링한 페이지를 그대로 보관하므로 콘텐츠가 제거되더라도 일정 시간동안 여전히 검색되고 접근 가능하다.

##### 동적 게이트웨이 접근

* 로봇이 그들이 접근하는 것에 대해 언제나 잘 아는 것은 아니다.
* 로봇은 게이트웨이 애플리케이션의 콘텐츠에 대한 URL로 요청을 할 수도 있다.
* 이 경우 얻은 데이터는 아마 특수 목적을 위한 것일 테고 처리 비용이 많이 들 수 있다.
* 많은 웹 사이트 관리자가 게이트웨이에서 얻은 문서를 요청하는 순진한 로봇을 좋아하지 않는다.

## 9.4 로봇 차단하기

* 1994년, 'Robots Exclusion Standard'라는 로봇이 적절한 장소에 들어가고 웹 마스터가 그들 동작을 더 잘 제어할 수 있는 메커니즘을 제공하는 기법이 제안되었다.
  * 로본 접근을 제어하는 정보를 저장하는 파일 이름을 따라 그냥 `robots.txt`라고 불린다.
* 웹 서버의 문서 루트에 `robots.txt`라고 이름 붙은 선택적 파일을 제공하여, 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보를 담는다.
* 로봇이 이 표준을 따른다면 웹 사이트의 다른 리소스에 접근하기 전에, 우선 그 사이트의 `robots.txt`를 요청하여 권한을 확인하고 필요한 페이지의 리소스를 가져오게 된다.

### 9.4.1 로봇 차단 표준

* 로봇 차단 표준은 임시방편으로 로봇 접근 제어 능력이 불완전한 상태이지만 없는 것보다 훨씬 낫고, 대부분 주류 업체와 검색엔진 크롤러가 이 표준을 지원한다.
* 로봇 차단 표준의 버전
  * `v0.0`: 로봇 배제 표준-Disallow 지시자를 지원하는 마틴 코스터의 오리지널 `robots.txt` 메커니즘 (1994년 6월)
  * `v1.0`: 웹 로봇 제어 방법-Allow 지시자의 지원이 추가된 마틴 코스터의 IETF 초안 (1996년 11월)
  * `v2.0`: 로봇 차단을 위한 확장 표준-정규식과 타이밍 정보를 포함한 숀 코너의 확장. 널리 지원되지 않음 (1996년 11월)
* 오늘날 대부분 로봇은 v0.0이나 v1.0 표준을 채택한다. v2.0 표준은 훨씬 복잡하고 널리 채택되지 못하고 있다.

### 9.4.2 웹 사이트와 robots.txt 파일들

* 호스트 명과 포트번호에 의해 정의되는 어떤 웹 사이트가 있을 때, 그 사이트 전체에 대한 `robots.txt` 파일은 단 하나만 존재해야 한다.
* 웹 사이트가 가상호스팅된다면, 다른 모든 파일이 그러하듯 각각의 가상 docroot에 서로 다른 `robots.txt`가 있을 수 있다.
* `robots.txt`가 반드시 파일시스템에 존재해야 하는 것은 아니다. 예를 들어 게이트웨이 애플리케이션이 `robots.txt`를 동적으로 생성할 수도 있다.

##### robots.txt 가져오기

* 로봇은 웹 서버 다른 파일들처럼 HTTP GET 메서드로 `robots.txt` 리소스를 가져온다. `robots.txt`가 존재하면 서버는 그 파일을 text/plain 본문으로 반환한다.
* 서버가 404 Not Found HTTP 상태 코드로 응답하면 그 서버는 로봇 접근을 제한하지 않는 것으로 간주하여 로봇은 어떤 파일이든 요청하게 된다.
* 로봇은 사이트 관리자가 로봇 접근을 추적할 수 있도록 From이나 User-Agent 헤더를 통해 신원 정보를 넘기고, 사이트 관리자가 로봇에 대해 문의나 불만사항이 있을 경우를 위해 연락처를 제공해야 한다.
* 상용 웹 로봇이 보낼 수 있는 HTTP 크롤러 요청의 예
  ```
  GET /robots.txt HTTP/1.0
  Host: www.abc.com
  User-Agent: Slurp/2.0
  Date: Wed Oct 2 20:22:35 EST 2020
  ```

##### 응답코드

* 많은 웹 사이트가 `robots.txt`를 갖고 있지 않지만 로봇은 그 사실을 모른다.
* 로봇은 어떤 웹 사이트든 반드시 `robots.txt`를 찾아보고 그의 검색 결과에 따라 다르게 동작한다.
  * 서버가 성공(HTTP 상태코드 `200`)으로 응답하면 로봇은 반드시 그 응답의 콘텐츠를 파싱하여 차단 규칙을 얻고, 그 사이트에서 무언가 가져오려 할 때 그 규칙에 따라야 한다.
  * 리소스가 존재하지 않는다고 서버가 응답하면(HTTP 상태 코드 `404`) 로봇은 활성화된 차단 규칙이 존재하지 않는다고 가정하고 `robots.txt`의 제약 없이 그 사이트에 접근할 수 있다.
  * 서버가 접근 제한(HTTP 상태 코드 `401` 혹은 `403`)으로 응답하면 로봇은 그 사이트로의 접근은 완전히 제한되어 있다고 가정해야 한다.
  * 요청 시도가 일시적으로 실패했다면(HTTP 상태코드 `503`) 로봇은 그 사이트의 리소스를 검색하는 것은 뒤로 미뤄야 한다.
  * 서버 응답이 리다이렉션을 의미하면(HTTP 상태코드 `3XX`) 로봇은 리소스가 발견될 때가지 리다이렉트를 따라가야 한다.

### 9.4.3 robots.txt 파일 포맷

* `robots.txt` 파일은 매우 단순한 줄 기반 문법을 갖는다.
* `robots.txt` 파일의 각 줄은 빈 줄, 주석 줄, 규칙 줄의 세 가지 종류가 있다.
* 규칙줄: HTTP 헤더처럼 생겼고(`<필드>:<값>`) 패턴 매칭을 위해 사용한다.

```
# 이 robots.txt는 slurp과 webcrawler의 크롤링은 허락하나 다른 로봇은 안 된다.
# 추가로 다른 로봇들이 이 사이트의 그 무엇에도 접근할 수 없게 막는다.

User-Agent: slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: *
Disallow:
```

* `robots.txt`의 이 줄들은 레코드로 구분된다. 각 레코드는 특정 로봇들의 집합에 대한 차단 규칙의 집합을 기술한다. 이를 통해 로봇별로 각각 다른 차단 규칙을 적용할 수 있다.
* 각 레코드는 규칙 줄들의 집합으로 되어 있으며 빈 줄이나 파일 끝(end-of-file) 문자로 끝난다.
* 레코드는 어떤 로봇이 이 레코드에 영향을 받는지 지정하는 하나 이상의 User-Agent 줄로 시작하며 뒤이어 이 로봇들이 접근할 수 있는 URL들을 말해주는 Allow 줄과 Disallow 줄이 온다.

##### User-Agent 줄

* 각 로봇의 레코드는 하나 이상의 User-Agent 줄로 시작한다.

```
User-Agent: <robot-name>
```
```
User-Agent: *
```

* 로봇의 이름(로봇 구현자에 의해 정해진)은 로봇의 HTTP GET 요청 안의 User-Agent 헤더를 통해 보내진다.
* `robots.txt` 파일을 처리한 로봇은 다음의 레코드에 반드시 복종해야 한다.
  * 로봇 이름이 자신 이름의 부분 문자열이 될 수 있는 레코드들 중 첫 번째 것.
  * 로봇 이름이 `*`인 레코드들 중 첫 번째 것.
* 로봇이 자신의 이름에 대응하는 User-Agent 줄을 찾지 못하였고 와일드카드를 사용한 `User-Agent: *` 줄도 찾지 못하면, 대응하는 레코드가 없는 것이므로 접근에 어떤 제한도 없다.
* 로봇 이름을 대소문자 구분 없는 부분 문자열과 맞춰보므로, 의도치 않게 맞는 경우에 주의해야 한다.
  * 예) `User-Agent: bot`은 Bot, Robot, Bottom-Feeder, Spambot, Dont-Bother-Me에 매치된다.

##### Disallow와 Allow 줄들

* Disallow와 Allow 줄은 로봇 차단 레코드의 User-Agent 줄들 바로 다음에 온다.
* 특정 로봇에 대해 어떤 URL 경로가 명시적으로 금지/허용되는지 기술한다.
* 로봇은 반드시 요청하려는 URL을 차단 레코드의 모든 Disallow와 Allow 규칙에 순서대로 맞춰 보아야 한다. 첫 번째로 맞은 것이 사용된다. 어떤 것도 맞지 않으면 그 URL을 허용된다.
* URL과 맞는 하나의 Disallow/Allow 줄에 대해, 규칙 경로는 반드시 맞춰보고자 하는 경로의 대소문자를 구분하는 접두어여야 한다.
  * 예) `Disallow: /tmp`는 다음 모든 URL에 대응된다.
    ```
    http://www.abc.com/tmp
    http://www.abc.com/tmp/
    http://www.abc.com/tmp/pliers.html
    http://www.abc.com/tmpspc/stuff.txt

    ```

##### Disallow/Allow 접두 매칭(prefix matching)

* Disallow/Allow 규칙이 어떤 경로에 적용되려면, 그 경로 시작부터 규칙 경로의 길이만큼 문자열이 규칙 경로와 같아야 한다(대소문자 차이도 없어야 한다). User-Agent 줄과 달리 별표(`*`)는 특별한 의미를 갖지 않지만, 대신 빈 문자열을 이용해 모든 문자열에 매치되도록 할 수 있다.
* 규칙 경로나 URL 경로의 임의의 '이스케이핑된' 문자들(%XX)은 비교 전에 원래대로 복원된다.
  * 빗금(`/`)을 의미하는 `%2F`는 예외로, 반드시 그대로 매치되어야 한다.
* 어떤 규칙 경로가 빈 문자열이면, 그 규칙은 모든 URL 경로와 매치된다.

robots.txt 경로와 URL 경로

| 규칙 경로 | URL 경로 | 매치되는가? | 부연 설명 |
|:---|:---|:---:|:---|
| `/tmp` | `/tmp` | O | 규칙 경로 == URL 경로 |
| `/tmp` | `/tmpfile.html` | O | 규칙 경로가 URL 경로의 접두어다. |
| `/tmp` | `/tmp/a.html` | O | 규칙 경로가 URL 경로의 접두어다. |
| `/tmp/` | `/tmp` | X | `/tmp/`는 `/tmp`의 접두어가 아니다. |
|  | README.TXT | O | 빈 문자열은 모든 것에 매치된다. |
| `/~fred/hi.html` | `%7Efred/hi.html` | O | `%7E`는 `~`과 같은 것으로 취급된다. |
| `/%7Efred/hi.html` | `~fred/hi.html` | O | `%7E`는 `~`과 같은 것으로 취급된다. |
| `/%7efred/hi.html` | `%7Efred/hi.html` | O | 이스케이핑된 문자는 대소문자를 구분하지 않는다. |
| `/~fred/hi.html` | `~fred%2Fhi.html` | X | `%2F`는 빗금(/)이 맞긴 하지만, 빗금은 특별히 정확하게 매치되어야 한다. |

* 접두 매칭은 꽤 잘 동작하지만, 그것으로는 충분하지 못한 경우가 몇 가지 있다.
  * 어떤 경로 밑에 있느냐와 상관없이 특정 이름의 디렉터리에 대해서는 크롤링을 막고 싶을 때, `robots.txt`는 이를 표현할 수단을 제공해주지 않는다.
  * `robots.txt` 버전 1.0 스킴은 모든 경로의 특정 하위 디렉터리를 일일이 지정하는 것 외에 별다른 방법이 없다.

### 9.4.4 그 외에 알아둘 점

* `robots.txt` 파일 파싱 규칙
  * `robots.txt` 파일 명세가 발전함에 따라 User-Agent, Disallow, Allow 외 다른 필드를 포함할 수 있다. 로봇은 자신이 이해하지 못하는 필드는 무시해야 한다.
  * 하위 호환성을 위해 한 줄을 여러 줄로 나누어 적는 것은 허용되지 않는다.
  * 주석은 파일의 어디서든 허용된다. 주석은 선택적인 공백 문자와 뒤이은 주석문자(`#`)로 시작해서, 그 뒤에 줄바꿈 문자가 나올 때까지 이어지는 주석 내용으로 이루어진다.
  * 로봇 차단 표준 버전 0.0은 Allow 줄을 지원하지 않는다. 오직 0.0 버전 명세만 구현하는 로봇은 Allow 줄들을 무시한다. 이 경우 로봇은 보수적으로 동작할 것이므로 허용되는 URL도 탐색하지 않을 수 있다.

### 9.4.5 robots.txt의 캐싱과 만료

* 매 파일 접근마다 로봇이 `robots.txt` 파일을 새로 가져와야 한다면, 로봇의 효율이 떨어질 뿐 아니라 웹 서버 부하도 늘어날 것이다.
* 대신 로봇은 주기적으로 `robots.txt`을 가져와서 그 결과를 캐시해야 한다.
* 로봇은 만료될 때까지 `robots.txt`의 캐시된 사본을 사용한다.
* `robots.txt` 파일의 캐싱 제어를 위해 원 서버와 로봇 양쪽 모두 표준 HTTP 캐시 제어 메커니즘을 사용한다.
* 로봇은 HTTP 응답의 Cache-Control과 Expires 헤더에 주의를 기울여야 한다.
* 오늘날 많은 크롤러 제품들은 HTTP/1.1 클라이언트가 아니라서 웹 마스터들은 이 크롤러들이 `robots.txt` 리소스에 적용된 캐시 지시자를 이해하지 못할 수도 있다는 점에 주의해야 한다.
* 로봇 명세 초안은 Cache-Control 지시자가 존재하는 경우 7일간 캐싱하도록 하고 있다. 그러나 실무에서 보면 이는 보통 너무 길다.
  * `robots.txt` 파일이 없는 상태가 1주일 동안 캐시되면, 새로 만들어진 `robots.txt` 파일은 아무 효과가 없을 것이고, 사이트 관리자는 로봇 차단 표준을 따르지 않는 로봇 관리자를 비난할 것이다.
  * 몇몇 대규모 웹 크롤러는 웹을 활발히 크롤링하는 기간 동안에는 `robots.txt`를 매일 새로 가져온다는 규칙을 따른다.

### 9.4.6 로봇 차단 펄 코드

* `robots.txt` 파일과 상호작용하는 공개된 펄(Perl) 라이브러리가 몇 가지 있다.
* 파싱된 `robots.txt` 파일이 담겨 있는 `WWW::RobotsRules` 객체는 주어진 URL에 대한 접근이 금지되어 있는지 확인할 수 잇는 메서드를 제공한다. 같은 `WWW::RobotsRules` 객체로 여러 `robots.txt` 파일을 파싱할 수 있다.
### 9.4.7 HTML 로봇 제어 META 태그

* `robots.txt` 파일의 단점 중 하나는 그 파일을 콘텐츠의 작성자 개개인이 아니라 웹 사이트 관리자가 소유한다는 것이다.
* HTML 페이지 저자는 로봇이 개별 페이지에 접근하는 것을 제한하는 좀 더 직접적인 방법을 갖고 있다.
  * HTML 문서에 직접 로봇 제어 태그를 추가할 수 있다.
  * 로봇 제어 HTML 태그에 따르는 로봇들은 여전히 문서를 가져올 수 있으나, 로봇 차단 태그가 존재한다면 그들은 그 문서를 무시할 것이다.
  * `robots.txt` 표준과 마찬가지로 따르는 것이 권장되지만 강제하지는 않는다.
* 로봇 차단 태그는 HTML META 태그를 이용한다.
  ```
  <MEAT NAME="ROBOTS" CONTENT=directive-list>
  ```

#### 로봇 META 지시자

* 로봇 META 지시자에 몇 가지 종류가 있으며, 점차 검색 엔진과 그들의 로봇이 활동과 기능 집합을 확장함께 따라 새 지시자가 추가될 가능성이 높다.

가장 널리 쓰이는 로봇 META 지시자 두 가지는 다음과 같다.
##### NOINDEX

* 로봇에게 이 페이지를 처리하지 말고 무시하라고 말해준다.
  * 예: 이 페이지 콘텐츠를 색인이나 데이터베이스에 포함시키지 말 것
  `<MEAT NAME="ROBOTS" CONTENT="NOINDEX">`

##### NOFOLLOW

* 로봇에게 이 페이지가 링크한 페이지를 크롤링하지 말라고 말해준다. `<MEAT NAME="ROBOTS" CONTENT="NOFOLLOW">`


NOINDEX와 NOFOLLOW 외의 지시자도 있다.
##### INDEX

* 로봇에게 이 페이지 콘텐츠를 인덱싱해도 된다고 말해준다.

##### FOLLOW

* 로봇에게 이 페이지가 링크한 페이지를 크롤링해도 된다고 말해준다.

##### NOARCHIVE

* 로봇에게 이 페이지의 캐시를 위한 로컬 사본을 만들어서는 안 된다고 말해준다.

##### ALL

* INDEX, FOLLOW 와 같다.

##### NONE

* NOINDEX, NOFOLLOW와 같다.


로봇 META 태그는 다른 모든 HTML META 태그처럼 반드시 HTML 페이지의 HEAD 섹션에 나타나야 한다.

* 이 태그에서 name과 content의 값은 대소문자를 구분하지 않는다는 것에 주의하라. 지시자들이 서로 충돌하거나 중복되게 해서는 당연히 안 된다. 이에 대한 동작은 정의되어 있지 않으며, 로봇 구현에 따라 제각각일 것이다.
##### 검색엔진 META 태그

* 모든 로봇 META 태그는 `name="robots"` 속성을 포함한다.
* 추가 META 태그 지시자
  * `<META name="DESCRIPTION" content={텍스트}>` : 웹 페이지의 짧은 요약을 정의
  * `<META name="KEYWORDS" content={쉼표 목록}>` : 키워드 검색을 돕기 위한 웹페이지를 기술하는 단어들의 쉼표로 구분되는 목록
  * `<META nmae="REVISIT-AFTER" content={숫자 days}>` : 로봇이나 검색엔진에게, 이 페이지는 아마도 쉽게 변경될 것이기 때문에 지정한 만큼의 날짜가 지난 후에 다시 방문해야 한다고 지시한다.
    * 그다지 널리 지원되지 않는 듯 하다.

## 9.5 로봇 에티켓

* 1993년, 웹 로봇 커뮤니티 개척자인 마틴 코스터는 웹 로봇을 만드는 사람들을 위한 가이드라인 목록을 작성했다: [로봇 제작자들을 위한 가이드라인](https://www.robotstxt.org/guidelines.html)
* 원본을 기반으로 현대적으로 고쳐 쓴 가이드라인은 작은 규모의 크롤러에도 적용할 수 있다. (참고: 이 책 p277 표9-6)

## 9.6 검색엔진

* 웹 로봇을 가장 광범위하게 사용하는 것은 인터넷 검색엔진이다.
* 오늘날 가장 유명한 웹 사이트 상당수가 인터넷 검색엔진이며, 이는 많은 웹 사용자들의 시작점인 동시에, 사용자가 전 세계 어떤 주제에 대한 문서라도 찾을 수 있게 한다.
* 웹 크롤러들은 먹이를 주듯 웹에 존재하는 문서들을 가져다주어, 검색엔진이 어떤 문서에 어떤 단어들이 존재하는지에 대한 색인을 생성할 수 있게 한다.

### 9.6.1 넓게 생각하라

* 웹 초창기의 검색엔진은 사용자들이 웹상의 문서 위치를 알아내는 것을 돕는 상대적으로 단순한 데이터베이스였다.
* 수백만 명의 사용자들이 수십억 개 웹페이지에서 원하는 정보를 찾을 수 있는 오늘날, 검색엔진은 필수가 되었고 수많은 웹페이지들을 검색하기 위해 복잡한 크롤러를 사용하게 되었다.
* 검색 색인이 필요로하는 페이지들을 검색하기 위해 수십억 개의 HTTP 질의를 생성하는 크롤러를 예로, 요청 완료에 각각 0.5초가 걸린다고 가정했을 때 10억 개의 문서를 다루면 대략 5,700일이 걸린다.
* 대규모 크롤러가 작업을 완료하려면 많은 장비를 똑똑하게 이용하여 요청을 병렬로 수행할 수 있어야할 것이다. 그러나 그 규모 때문에 웹 전체를 크롤링하는 것은 여전히 쉽지 않은 도전이다.

### 9.6.2 현대적인 검색엔진의 아키텍쳐

* 오늘날 검색엔진은 그들이 가진 전 세계 웹페이지들에 대해 '풀 텍스트 색인(full-text indexes)'이라고 하는 복잡한 로컬 데이터베이스를 생성한다.
  * 이 색인은 웹의 모든 문서에 대해 일종의 카드 카탈로그처럼 동작한다.
* 검색엔진 크롤러들은 웹페이지들을 수집하여 가져와서 이 풀 텍스트 색인에 추가한다. 동시에 검색엔진 사용자들은 구글과 같은 검색 게이트웨이를 통해 풀 텍스트 색인에 대한 질의를 보낸다.
* 크롤링을 한 번 하는데 걸리는 시간이 상당한 데 비해 웹페이지들은 매 순간 변화하므로, 풀 텍스트 색인은 기껏 해야 웹의 특정 순간에 대한 스냅숏에 불과하다.

### 9.6.3 풀 텍스트 색인

* 풀 텍스트 색인은 단어 하나를 입력받아 그 단어를 포함하는 문서를 즉각 알려줄 수 있는 데이터베이스다.
* 이 문서들은 색인 생성 후에는 검색할 필요가 없다.
* 문서 A, B, C가 있을 경우, 세 문서 안의 모든 단어에 대해 각각을 포함한 문서를 열거한다.
  * 단어 'a'는 문서 A와 B에 들어있다.
  * 단어 'best'는 문서 A와 C에 들어있다.
  * 단어 'the'는 세 문서 A, B, C 모두에 들어있다.

### 9.6.4 질의 보내기

* HTML 폼을 사용자가 채워 넣고 브라우저가 그 폼을 HTTP GET이나 POST 요청을 이용해 게이트웨이로 보내는 식으로, 사용자가 웹 검색엔진 게이트웨이에 질의를 보낸다.
* 게이트웨이 프로그램은 검색 질의를 추출하고 웹 UI 질의를 풀 텍스트 색인을 검색할 때 사용되는 표현으로 변환한다.
* 검색 질의 요청의 예
  ```
  // 요청
  GET /search.html?query=drills HTTP/1.1
  Host: www.abc.com
  Accept: *
  User-agent: ShopBot

  // 응답
  HTTP/1.1 200 OK
  Content-type: text/html
  Content-length: 1037

  <HTML>
  <HEAD><TITLE>Search Results</TITLE>
  <A HREF="/BD.html">Black and Decker Drills<A>
  [...]
  ```

### 9.6.5 검색 결과를 정렬하고 보여주기

* 질의 결과를 확인하기 위해 검색엔진이 색인을 한번 사용했다면, 게이트웨이 애플리케이션은 그 결과를 이용해 최종 사용자를 위한 결과 페이지를 즉석에서 만들어낸다.
* 많은 웹페이지가 주어진 단어를 포함할 수 있기 때문에, 검색엔진은 결과에 순위를 매기기 위해 똑똑한 알고리즘을 사용한다.
  * 예) 관련도 랭킹(relevancy ranking): 문서들이 주어진 단어와 가장 관련이 많은 순서대로 결과 문서에 나타날 수 있도록, 문서들 간의 순서를 알아서 검색 결과의 목록에 점수를 매기고 정렬함
* 이 과정을 더 잘 지원하기 위해, 많은 검색엔진은 웹을 크롤링하는 과정에서 수집된 통계 데이터를 실제로 사용한다.
  * 주어진 페이지를 가리키는 링크가 얼마나 많은지 세어 그 문서의 인기도를 판별하고 결과를 보여줄 때 정렬 순서에 대한 가중치로 사용할 수 있다.
* 검색엔진에 의해 사용되는 알고리즘, 크롤링에 대한 팁, 각종 기교는 검색엔진의 가장 엄격히 감추어진 비밀이다.
### 9.6.6 스푸핑

* 사용자들은 자신이 찾는 내용이 검색 결과의 상위 몇 줄 안에 보이지 않으면 대개 불만족스러워 하므로, 검색 결과의 순서는 중요하다.
* 웹 마스터는 자신이 만든 사이트가 그 자신을 잘 설명하는 단어로 검색한 결과 상단에 노출되도록 만들 동기가 충분하다.
* 검색 결과에서 더 높은 순위를 차지하고자 하는 바람은 검색 시스템과 게임으로 이어져, 검색엔진과 자신의 사이트를 눈에 띄게 할 방법을 찾는 이들 사이의 끝없는 줄다리기를 만들어냈다.
  * 수많은 관련없는 키워드를 나열하여 가짜 페이지를 만들거나, 검색엔진 관련도 알고리즘을 더 잘 속일 수 있는 특정 단어에 대한 가짜 페이지를 생성하는 게이트웨이 애플리케이션을 만들기도 한다.
* 검색엔진과 로봇 구현자들은 이런 속임수를 더 잘 잡아내기 위해 끊임없이 그들의 관련도 알고리즘을 수정해야 한다.

